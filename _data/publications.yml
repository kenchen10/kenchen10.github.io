- id: vr25
  title: "Perceptually-Guided Acoustic ``Foveation''"
  venue: IEEE VR
  venue_long: Conference Proceedings
  year: 2025
  showyear: 2025
  description: We present a perceptual sound source clustering technique.
  project_page: "projects/vr25/index.html"
  github: 
  arxiv: 
  pdf: "assets/papers/audio_foveation-1.pdf"
  teaser_vid: 
  image: vr25.png
  bgcolor: 
  tag: "#C5CAE9"
  teaser_image: vr25_teaser-modified.png
  yt_emb: 
  video: 
  teaser_text: "Figure 1: Azimuth-based audio perceptual acuity guided sound source clustering. (a) visualizes our model-predicted human auditory perception of spatial discrimination threshold along azimuth eccentricity in degrees. (b) illustrates the model-derived audio source clustering method. Based on listeners’ heading direction (assuming forward here), we cluster audio sources that are spatially indistinguishable. Clusters were highlighted by colors corresponding to the human’s minimum audible angle. The number of sound sources for each cluster is marked in the figure. Our measurement shows a 53% computational saving at the presented scene."
  authors:
    - xpeng
    - kchen
    - iroman
    - jpbello
    - qsun
    - pchakravarthula
  equal_adv:
    - pchakravarthula
    - qsun
  affiliation_images:
    - nyu.png
    - marl.png
    - UNC-Logo.png
  bibtex:
  abstract: "Realistic spatial audio rendering improves immersion in virtual environments. However, the computational complexity of acoustic propagation increases linearly with the number of sources. Consequently, real-time accurate acoustic rendering becomes challenging in highly dynamic scenarios such as virtual and augmented reality (VR/AR). Exploiting the fact that human spatial sensitivity of acoustic sources is not equal at azimuth eccentricities in the horizontal plane, we introduce a perceptually-aware acoustic “foveation” guidance model to the audio rendering pipeline, which can integrate audio sources that are not spatially resolvable by human listeners. To this end, we first conduct a series of psychophysical studies to measure the minimum resolvable audible angular distance under various spatial and background conditions. We leverage this data to derive an azimuth-characterized real-time acoustic foveation algorithm. Numerical analysis and subjective user studies in VR environments demonstrate our method’s effectiveness in significantly reducing acoustic rendering workload, without compromising users’ spatial perception of audio sources. We believe that the presented research will motivate future investigation into the new frontier of modeling and leveraging human multimodal per- ceptual limitations — beyond the extensively studied visual acuity — for designing efficient VR/AR systems."

- id: sig24
  title: "<b style='color:#4CBB17;'>PEA-PODs</b>: <b style='color:#4CBB17;'>P</b>erceptual <b style='color:#4CBB17;'>E</b>valuation of <b style='color:#4CBB17;'>A</b>lgorithms for <b style='color:#4CBB17;'>P</b>ower <b style='color:#4CBB17;'>O</b>ptimization in XR <b style='color:#4CBB17;'>D</b>isplay<b style='color:#4CBB17;'>s</b>"
  venue: ACM SIGGRAPH
  venue_long: Journal Proceedings
  year: 2024
  showyear: 2024
  description: "We conduct user studies to evaluate display power optimization algorithms."
  project_page: "projects/sig24/index.html"
  github: NYU-ICL/pea-pods.git
  arxiv: 
  poster: assets/PEAPODs-Poster.pdf
  pdf: assets/papers/peapods_manuscript_Part1.pdf
  supp: assets/papers/peapods_manuscript_Part2.pdf
  papersite: 
  teaser_vid: 
  award: Best Paper Award (Honorable Mention)
  award_link: https://blog.siggraph.org/2024/06/siggraph-2024-technical-papers-awards-best-papers-honorable-mentions-and-test-of-time.html/
  image: pea-pods-teaser2.png
  bgcolor: True
  tag: "#FFCCBC"
  teaser_image: peapods_teaser.png
  yt_emb: https://www.youtube.com/embed/CcITr7fy8Go
  video: https://youtu.be/CcITr7fy8Go
  teaser_text: We studied six power-saving display mapping algorithms. These techniques are used in traditional display power reduction as well as recently-proposed methods for VR displays. Just objectionable difference (JOD) scores provide a unified measure of the magnitude of perceptual impact, and percentage values represent relative display power savings (OLED display shown here). Optimal techniques have low perceptual impact (perceived close to reference in JODs), but provide big power savings.
  authors:
    - kchen
    - twan
    - nmatsuda
    - aninan
    - achapiro
    - qsun
  equal_adv:
    - achapiro
    - qsun
  affiliation_images:
    - nyu.png
    - meta.png
  bibtex: "
  @article{ <br>
    <tab>chen2024peapods,<br>
    <tab>author = {Chen, Kenneth and Wan, Thomas and Matsuda, Nathan and Ninan, Ajit and Chapiro, Alexandre and Sun, Qi},<br>
    <tab>title = {PEA-PODs: Perceptual Evaluation of Algorithms for Power Optimization in XR Displays},<br>
    <tab>year = {2024},<br>
    <tab>issue_date = {July 2024},<br>
    <tab>publisher = {Association for Computing Machinery},<br>
    <tab>address = {New York, NY, USA},<br>
    <tab>volume = {43},<br>
    <tab>number = {4},<br>
    <tab>issn = {0730-0301},<br>
    <tab>url = {https://doi.org/10.1145/3658126},<br>
    <tab>doi = {10.1145/3658126},<br>
    <tab>journal = {ACM Trans. Graph.},<br>
    <tab>month = {jul},<br>
    <tab>articleno = {67},<br>
    <tab>numpages = {17},<br>
  }
  "
  abstract: "Display power consumption is an emerging concern for untethered devices. This goes double for augmented and virtual extended reality (XR) displays, which target high refresh rates and high resolutions while conforming to an ergonomically light form factor.
A number of image mapping techniques have been proposed to extend battery usage. However, there is currently no comprehensive quantitative understanding of how the power savings provided by these methods compare to their impact on visual quality. We set out to answer this question.
<br><br>
To this end, we present a <b>p</b>erceptual <b>e</b>valuation of <b>a</b>lgorithms (PEA) for <b>p</b>ower <b>o</b>ptimization in XR <b>d</b>isplay<b>s</b> (PODs). Consolidating a portfolio of six power-saving display mapping approaches, we begin by performing a large-scale perceptual study to understand the impact of each method on perceived quality in the wild. This results in a unified quality score for each technique, scaled in just-objectionable-difference (JOD) units.
In parallel, each technique is analyzed using hardware-accurate power models.
<br><br>
The resulting JOD-to-Milliwatt transfer function provides a first-of-its-kind look into tradeoffs offered by display mapping techniques, and can be directly employed to make architectural decisions for power budgets on XR displays. 
Finally, we leverage our study data and power models to address important display power applications like the choice of display primary, power implications of eye tracking, and more."

- id: asplos24
  title: "Exploiting Human Color Discrimination for Memory- and Energy-Efficient Image Encoding in Virtual Reality"
  venue: ACM ASPLOS
  # venue_long: ACM International Conference on Architectural Support for Programming Languages and Operating Systems
  year: 2024
  showyear: 2024
  description: "A DRAM traffic compression scheme using peripheral color modulation."
  project_page: "projects/asplos24/index.html"
  github: horizon-research/hvs_vr_encoding.git
  arxiv: 
  papersite: https://horizon-lab.org/pubs/asplos24-vr.pdf
  teaser_vid: 
  image: asplos24-2.png
  bgcolor: 
  tag: "#E1BEE7"
  teaser_image: asplos24teaser.png
  yt_emb: 
  video: 
  teaser_text: Our algorithm takes a tile of pixels and their corresponding discrimination ellipsoid parameters, and generate an adjusted pixel tile, which then goes through existing BD encoding.
  authors:
    - nujjainkar
    - eshahan
    - kchen
    - bduinkharjav
    - qsun
    - yzhu
  affiliation_images:
    - uofr.png
    - nyulogo.png
  bibtex: "@inproceedings{<br>
    <tab>ujjainkar2024exploiting, <br>
    <tab>author = {Ujjainkar, Nisarg and Shahan, Ethan and Chen, Kenneth and Duinkharjav, Budmonde and Sun, Qi and Zhu, Yuhao}, <br>
    <tab>title = {Exploiting Human Color Discrimination for Memory- and Energy-Efficient Image Encoding in Virtual Reality}, <br>
    <tab>year = {2024}, <br>
    <tab>url = {https://doi.org/10.1145/3617232.3624860}, <br>
    <tab>booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1}, <br>
    <tab>pages = {166–180}, <br>
    <tab>numpages = {15}, <br>
    <tab>series = {ASPLOS '24}<br>
    }"
  abstract: "Virtual Reality (VR) has the potential of becoming the next ubiquitous computing platform. Continued progress in the burgeoning field of VR depends critically on an efficient computing substrate. In particular, DRAM access energy is known to contribute to a significant portion of system energy. Today’s framebuffer compression system alleviates the DRAM traffic by using a numerically lossless compression algorithm. We observe that being numerically lossless is unnecessary to preserve perceptual quality for users. This paper proposes a perceptually lossless, but numerically lossy, system to compress DRAM traffics. Our idea builds on top of long-established psychophysical studies that humans cannot discriminate colors that are close to each other. The discrimination ability becomes even weaker (i.e., more colors are perceptually undistinguishable) in our peripheral vision. Leveraging the color discrimination (in)ability, we propose an algorithm that adjusts pixel colors to minimize the bit encoding cost without introducing visible artifacts. The algorithm is coupled with lightweight architectural support that, in real-time, reduces the DRAM traffic by 66.9% and outperforms existing framebuffer compression mechanisms by up to 20.4%. Psychophysical studies on human participants show that our system introduce little to no perceptual fidelity degradation."

- id: sca23
  title: "Towards Learning and Generating Audience Motion from Video"
  venue: ACM SCA
  venue_long: ACM SIGGRAPH/Eurographics Symposium on Computer Animation, Poster
  year: 2023
  description: "A dataset of audience videos."
  project_page: "projects/sca23/index.html"
  github: 
  arxiv: 
  pdf: assets/papers/sca23-4.pdf
  image: audiences-2.png
  bgcolor: 
  tag: "#C8E6C9"
  paper_image: 
  teaser_image: datagenpipeline-1.png
  yt_emb: 
  video: 
  hide: true
  authors:
    - kchen
    - nbadler
  affiliation_images:
    - Cesium_dark_color.png
  bibtex: "@inproceedings{<br>
  <tab>chen2023towards,<br>
  <tab>author = {Chen, Kenneth and Badler, Norman},<br>
  <tab>title = {Towards Learning and Generating Audience Motion from Video},<br>
  <tab>year = {2023},<br>
  <tab>url = {https://doi.org/10.1145/3606037.3606839},<br>
  <tab>booktitle = {Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation},<br>
  <tab>articleno = {4},<br>
  <tab>numpages = {2},<br>
  <tab>series = {SCA '23}<br>
}"
  abstract: "There has recently been an explosion of interest in creating large-scale shared virtual spaces for multiplayer content. However, rendering player-controllable avatars in real-time creates latency issues when scaling to thousands of players. We introduce a human audience video dataset to support applications in deep learning-based 2D video audience simulation, bypassing the need for background 3D virtual humans. This dataset consists of YouTube videos that depict audiences with diverse lighting conditions, color, dress, and movement patterns. We describe the dataset statistics, our implicit data collection strategy, and audience video extraction pipeline. We apply deep learning tasks on this data based on video prediction techniques, and propose a novel method for 2D audience simulations."


- id: vr-energy-etech
  title: "Imperceptible Color Modulation for Power Saving in VR/AR"
  venue: ACM SIGGRAPH
  venue_long: Emerging Technologies
  year: 2023
  description: "A demo of the display setup and model from our SIGGRAPH Asia '22 paper."
  project_page: "projects/vr-energy-etech/index.html"
  github: 
  arxiv: 
  hide: true
  papersite: https://www.immersivecomputinglab.org/wp-content/uploads/2023/05/VR_Energy_E_Tech.pdf
  image: etech23-2.png
  bgcolor: 
  tag: "#FFCCBC"
  paper_image: vr-energy-etech.png
  teaser_image: etech23teaser.png
  yt_emb: 
  video: 
  teaser_text: a) We develop a hardware prototype OLED VR display to capture power measurements when displaying different images. b) Participants view a panoramic scene in VR with our filter turned on, and are encouraged to rotate freely to view the scene in a natural manner. c) Filter intensity is increased over 10 seconds. Image credits to VR Gorilla.
  authors:
    - kchen
    - bduinkharjav
    - nujjainkar
    - eshahan
    - atyagi
    - jhe
    - yzhu
    - qsun
  affiliation_images:
    - nyulogo.png
    - uofr.png
  bibtex: "@inproceedings{  <br>
    <tab>chen2023imperceptible,<br>
    <tab>author = {Chen, Kenneth and Duinkharjav, Budmonde and Ujjainkar, Nisarg and Shahan, Ethan and Tyagi, Abhishek and He, Jiayi and Zhu, Yuhao and Sun, Qi},<br>
    <tab>title = {Imperceptible Color Modulation for Power Saving in VR/AR},<br>
    <tab>year = {2023},<br>
    <tab>url = {https://doi.org/10.1145/3588037.3595388},<br>
    <tab>booktitle = {ACM SIGGRAPH 2023 Emerging Technologies},<br>
    <tab>articleno = {8},<br>
    <tab>numpages = {2},<br>
    <tab>series = {SIGGRAPH '23}<br>
  }"
  abstract: "Untethered VR/AR HMDs can only last 2-3 hours on a single charge. Toward resolving this issue, we develop a real-time gaze-contingent power saving filter which modulates peripheral pixel color while preserving visual fidelity. At SIGGRAPH 2023, participants will be able to view a short panoramic video within a VR HMD with our perceptually-aware power saving filter turned on. Participants will also have the opportunity to view the power output of scenes through our power measurement setup."


- id: emg-energy
  title: "Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck Muscle Contraction"
  venue: ACM SIGGRAPH
  venue_long: Conference Proceedings
  # banner_venue: siggraph.jpg
  year: 2023
  showyear: 2023
  description: "Leveraging EMG devices, we developed a model to predict VR neck muscular workload."
  project_page: "projects/emg-energy/index.html"
  github: NYU-ICL/xr-ergonomics-neck-comfort
  arxiv: 
  papersite: https://www.immersivecomputinglab.org/wp-content/uploads/2023/05/xr-ergonomics-neck-comfort.pdf
  teaser_vid: 
  image: emg-2.png
  bgcolor: 
  tag: "#FFCCBC"
  paper_image: mcl-paper.png
  teaser_image: teasersig23.png
  teaser_text: (a) A VR user chooses between two candidate head motion trajectories of seemingly similar muscular workload for a visual task. (b) Our computational model predicts the user’s potential neck muscle contraction level and thus perceived neck muscle discomfort before the movements happen. 3D asset credits to Mixall, Bizulka, RootMotion at Unity, and shockwavegamez01, joseVG at Sketchfab.
  yt_emb: https://www.youtube.com/embed/XO8VR1tJoaI
  video: https://www.youtube.com/watch?v=XO8VR1tJoaI
  authors:
    - yzhang
    - kchen
    - qsun
  affiliation_images:
    - nyulogo.png
  bibtex: "@inproceedings{<br>
    <tab>zhang2023toward,<br>
    <tab>title={Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck Muscle Contraction},<br>
    <tab>author={Zhang, Yunxiang and Chen, Kenneth and Sun, Qi},<br>
    <tab>booktitle={ACM SIGGRAPH 2023 Conference Proceedings},<br>
    <tab>pages={1–12},<br>
    <tab>year={2023}<br>
}"
  abstract: "Ergonomic-friendly usage is essential to mass and prolonged adoption of virtual/augmented reality (VR/AR) head-mounted displays (HMDs). Unlike conventional displays, VR/AR HMDs unlock users' wide-range, frequent, and natural head movements for viewing. Although neck comfort is inevitably compromised due to HMDs' hardware weight, we still have little quantitative knowledge of the resulting additional muscular workload.
<br><br>
Leveraging electromyography devices, we measure, model, and predict users' neck muscle contraction level while they rotate their heads to interact with surrounding objects in VR. Specifically, learning from the data obtained in our physiological pilot study, we establish a bio-physically inspired model for both stationary and dynamic head status. It 1) models quantified muscle contraction level given a complete head motion trajectory, and 2) predicts potential discomfort before a head movement occurs. We validate our model with a series of objective evaluation and user study. The results demonstrate its prediction accuracy for unseen movements, and capability in reducing muscular efforts and thus discomfort by altering the layout of virtual targets. We hope this research will motivate new ergonomic-centered designs and metrics for VR/AR and interactive computer graphics applications."

- id: vrenergy
  title: "Color-Perception-Guided Display Power Reduction for Virtual Reality"
  # venue2: SIGGRAPH
  # venue_long2: Emerging Technologies
  # year2: 2023
  affiliation_images:
    - nyulogo.png
    - uofr.png
  venue: ACM SIGGRAPH Asia
  venue_long: Journal Proceedings
  showyear: 2022
  year: 2022
  description: "We measured human sensitivity to peripheral color stimuli via a psychophysical studied, and applied this data to OLED display power optimization."
  project_page: "projects/vrenergy/index.html"
  # project_page2: "projects/vr-energy-etech/index.html"
  # project_page2_name: E-Tech
  github: NYU-ICL/vr-power-saver
  papersite: https://www.immersivecomputinglab.org/wp-content/uploads/2024/11/vr_energy_siga_22.pdf
  # arxiv: "2209.07610"
  # teaser_vid: sigasia.mp4
  image: colorfov-2.png
  bgcolor: True
  tag: "#BBDEFB"
  # tag2: "#FFCCBC"
  teaser_text: "We present a perceptually-guided, real-time, and closed-form model for minimizing the power consumption of untethered VR displays while preserving visual fidelity. We apply a gaze-contingent shader onto the original frame ((a) left) to produce a more power efficient frame ((a) right) while preserving the luminance level and perceptual fidelity during active viewing. The dashed circles indicate the user's gaze. Our method is jointly motivated by prior literature revealing that: i) the power cost of displaying different colors on LEDs may vary significantly, even if the luminance levels remain unchanged [Dong and Zhong 2011]; ii) human color sensitivity decreases in peripheral [Hansen et al. 2009] and active vision [Cohen et al. 2020]. The color palette of the original frame is modulated using our peripheral filter and is shown in (a) for a visual comparison. While the color palettes appear different when gazed upon, an observer cannot discriminate between them when shown in their periphery. (b) visualizes how our model shifts the image's chromatic histograms to minimize the physically-measured power consumption. The blue LEDs consume more energy than the red/green in our experiment display panel. Image credits to Tim Caynes © 2012."
  paper_image: vrenergy_paper.png
  teaser_image: sigasia22_teaser.png
  yt_emb: https://www.youtube.com/embed/bcbG_80TLa0
  video: https://www.youtube.com/watch?v=bcbG_80TLa0
  authors:
    - bduinkharjav
    - kchen
    - atyagi
    - jhe
    - yzhu 
    - qsun
  equal: 
    - bduinkharjav
    - kchen
  equal_adv:
    - yzhu
    - qsun
  bibtex: "@article{<br>
    <tab>duinkharjav2022color,<br>
    <tab>title = {Color-Perception-Guided Display Power Reduction for Virtual Reality},<br>
    <tab>author = {Duinkharjav, Budmonde and Chen, Kenneth and Tyagi, Abhishek and He, Jiayi and Zhu, Yuhao and Sun, Qi},<br>
    <tab>journal = {ACM Trans. Graph. (Proc. SIGGRAPH Asia)},<br>
    <tab>volume = {41},<br>
    <tab>number = {6},<br>
    <tab>pages = {144:1--144:16},<br>
    <tab>year = {2022}<br>
  }"
  abstract: "Battery life is an increasingly urgent challenge for today's untethered VR and AR devices.
However, the power efficiency of head-mounted displays is naturally at odds with growing computational requirements driven by better resolution, refresh rate, and dynamic ranges, all of which reduce the sustained usage time of untethered AR/VR devices.
For instance, Oculus Quest 2, under a fully-charged battery, can sustain only 2 to 3 hours of operation time.
Prior display power reduction techniques mostly target smartphone displays.
Directly applying smartphone display power reduction techniques, however, degrades the visual perception in AR/VR with noticeable artifacts.
For instance, the ``power-saving mode'' on smartphones <em>uniformly</em> lowers the pixel luminance across the display and, as a result, presents an overall darkened visual perception to users if directly applied to VR content.
<br><br>
Our key insight is that VR display power reduction must be cognizant of the gaze-contingent nature of high field-of-view VR displays. To that end, we present a gaze-contingent system that, without degrading luminance, minimizes the display power consumption while presenting an indiscriminable visual perception.
This is enabled by constructing 1) a gaze-contingent color discrimination model through psychophysical studies, and 2) a display power model (with respect to pixel color) through real-device measurements. Critically, due to the careful design decisions made in constructing the two models, our algorithm is cast as a constrained optimization problem with a <em>closed-form</em> solution, which can be implemented as a real-time, image-space shader.
We evaluate our system using a series of psychophysical studies and large-scale analyses on natural images.
Experiment results show that our system reduces the display power by as much as 24% (14% on average) with little to no perceptual fidelity degradation."
