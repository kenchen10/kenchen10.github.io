- id: sig25
  title: "What is HDR? Perceptual Impact of Luminance and Contrast in Immersive Displays"
  venue: SIGGRAPH
  doi: 10.1145/3721238.3730629
  year: 2025
  showyear: 2025
  description: "We studied how contrast/luminance compression affects user preferences in HDR."
  project_page: "projects/sig25/index.html"
  highlightvideo: sig25.mp4
  github: 
  arxiv: 
  poster: 
  pdf: assets/papers/What_is_HDR__Manuscript-13_Part1.pdf
  supp: assets/papers/What_is_HDR__Manuscript-13_Part2.pdf
  papersite: 
  teaser_vid: 
  award: 
  award_link: 
  image: WIH-image.png
  bgcolor: True
  tag: "#F6BEFF"
  teaser_image: WIH-teaser.png
  yt_emb: 
  video: images/wih_supp_video.mp4
  teaser_text: "A model to predict perceptual impact (in Just-Objectionable-Differences, or JODs) is derived from HDR preference data for combinations of display
contrast and peak luminance, with predictions visualized as a heatmap (left). In this plot, the baseline 0 JOD condition is set to values similar to commercially-
available VR displays: 100 nits peak luminance and 64:1 contrast. In addition, we simulate 3 displays with different dynamic ranges. Our model allows us
to examine the perceived improvement from increased peak luminance and contrast. For example, both display 2 and 3 provide a 1 JOD improvement
over display 1. Note that HDR content cannot be displayed in a PDF format, so all images in this manuscript are tone-mapped for presentation. See our
supplementary webpage for representative content."
  authors:
    - kchen
    - nmatsuda
    - jmcelvain
    - yzhao
    - twan
    - qsun
    - achapiro
  affiliation_images:
    - nyu.png
    - meta.png
  equal_adv:
    - achapiro
    - qsun
  bibtex: 
  abstract: "The contrast and luminance capabilities of a display are central to the quality
of the image. High dynamic range (HDR) displays have high luminance and
contrast, but it can be difficult to ascertain whether a given set of characteris-
tics qualifies for this label. This is especially unclear for new display modes,
such as virtual reality (VR). This paper studies the perceptual impact of peak
luminance and contrast of a display, including characteristics and use cases
representative of VR. To achieve this goal, we first developed a haploscope
testbed prototype display capable of achieving 1k nits peak luminance
and 1M:1 contrast with high precision. We then collected a novel HDR
video dataset targetting VR-relevant content types. We also implemented
custom tone mapping operators to map between display parameter sets.
Finally, we collected subjective preference data spanning 3 orders of mag-
nitude in each dimension. Our data was used to fit a model, which was
validated using a subjective study on an HDR VR prototype headmounted
display (HMD). Our model helps provide guidance for future display design,
and helps standardize the understanding of HDR."

- id: imggs
  title: "Image-GS: Content-Adaptive Image Representation via 2D Gaussians"
  venue: SIGGRAPH
  venue_long: 
  year: 2025
  highlightvideo: imggs.mp4
  description: "2D gaussians can serve as an effective technique for low bit rate image encoding."
  project_page: "projects/imggs/index.html"
  github: NYU-ICL/image-gs
  poster: 
  arxiv: 2407.01866
  supp: 
  papersite: 
  teaser_vid: 
  award: 
  award_link: 
  image: imggs.png
  tag: "#F6BEFF"
  teaser_image: imggs_teaser.png
  yt_emb: 
  video: 
  teaser_text: "Content-adaptive image representation with Image-GS. Leveraging a tailored differentiable renderer, Image-GS adaptively distributes and progressively
optimizes a set of 2D Gaussians to fit a target image. Image-GS shows high memory & computation e!iciency, supports fast random pixel access, and o!ers a
natural level of detail. (a) shows the learned Gaussian position distribution (green dots); 20% of Gaussians are plotted for better visibility. (b) Compared to
alternative methods, Image-GS's content-adaptive nature enables it to wisely allocate resources based on the local signal complexity and preserve fine image
details with higher fidelity. The insets visualize the corresponding error images, with brighter colors indicating higher errors."
  authors:
    - yzhang
    - bli
    - akuznetsov
    - ajindal
    - sdiolatzis
    - kchen
    - asochenov
    - akaplanyan
    - qsun
  equal: 
    - yzhang
    - bli
  affiliation_images:
    - nyu.png
    - intel.png
    - amd.png
  bibtex: 
  abstract: "Neural image representations have emerged as a promising approach for storing and rendering visual data. Combined with learning-based work-flows, these novel representations have demonstrated impressive balances
between visual quality and memory footprint. Existing methods along this line, however, often rely on fixed data structures that suboptimally allocate
memory budget or computation-intensive implicit neural models, limiting their adoption in real-time graphics applications.
Inspired by recent advances in radiance field rendering, we introduce Image-GS, an efficient, flexible, and content-adaptive image representation based on anisotropic 2D Gaussians. Image-GS delivers remarkable visual
quality and memory effciency while supporting fast random access and a
natural level-of-detail stack. Leveraging a custom differentiable renderer imlemented via efficient CUDA kernels, Image-GS reconstructs target images by adaptively allocating and progressively optimizing a set of 2D Gaussians.
Our method achieves superior visual fidelity over state-of-the-art neural image representations across diverse images and textures. Notably, Image-GS exhibits linear scaling in memory and computational requirements relative to the number of Gaussians, offering a flexible trade-off between fidelity and
run-time efficiency, which we demonstrate in machine vision and image restoration tasks."

- id: oe25
  title: "View Synthesis for 3D Computer-Generated Holograms Using Deep Neural Fields"
  venue: Optics Express
  venue_long: 
  year: 2025
  showyear:
  description: "Neural networks are trained to predict 3D holograms at novel scene views."
  project_page: "projects/oe25/index.html"
  github: 
  doi: 10.1364/OE.559364
  arxiv: 
  poster: 
  pdf: assets/papers/oe-33-9-19399.pdf
  supp: assets/papers/7444817.pdf
  papersite: 
  teaser_vid: 
  award: 
  award_link: 
  image: nhf.png
  tag: "#F6BEFF"
  teaser_image: nhf_teaser.png
  yt_emb: 
  video: images/supp-view-interp_s.mp4
  teaser_text: "(A) Camera captures are used to optimize a scene representation model. 
        (B) Light field elemental views are rendered by iteratively evaluating the radiance field at uniformly spaced camera positions.
        Epipolar slices show that the scene representation model can synthesize new views with high image quality.
        (C) The light field is converted to a complex wavefront by computing the inverse of the short-time Fourier transform (iSTFT).
        (D) The model weights of a CNN are optimized by backpropagating the errors of a focal stack loss.
        The ASM is used to simulate wavefront propagation at different distances from a reference plane.
        (E) Insets of model predictions for four test views are shown, with near and far focus."
  authors:
    - kchen
    - awen
    - yzhang
    - pchakravarthula
    - qsun
  affiliation_images:
    - nyu.png
    - UNC-Logo.png
  bibtex: "
    @article{chen2025neuralholographicfields,<br>
      <tab>author = {Kenneth Chen and Anzhou Wen and Yunxiang Zhang and Praneeth Chakravarthula and Qi Sun},<br>
      <tab>journal = {Opt. Express},<br>
      <tab>keywords = {Holographic displays; Imaging techniques; Light propagation; Neural networks; Spatial light modulators; Wave propagation},<br>
      <tab>number = {9},<br>
      <tab>pages = {19399--19408},<br>
      <tab>publisher = {Optica Publishing Group},<br>
      <tab>title = {View synthesis for 3D computer-generated holograms using deep neural fields},<br>
      <tab>volume = {33},<br>
      <tab>month = {May},<br>
      <tab>year = {2025},<br>
      <tab>url = {https://opg.optica.org/oe/abstract.cfm?URI=oe-33-9-19399},<br>
      <tab>doi = {10.1364/OE.559364}<br>
    }
  "
  abstract: "Computer-generated holography (CGH) simulates the propagation and interference of complex light waves, allowing it to reconstruct realistic images captured from a specific viewpoint by solving the corresponding Maxwell equations.
However, in applications such as virtual and augmented reality, viewers should freely observe holograms from arbitrary viewpoints, much as how we naturally see the physical world.  
In this work, we train a neural network to generate holograms at any view in a scene.
Our result is the Neural Holographic Field: the first artificial-neural-network-based representation for light wave propagation in free space and transform sparse 2D photos into holograms that are not only 3D but also freely viewable from any perspective. We demonstrate by visualizing various smartphone-captured scenes from arbitrary six-degree-of-freedom viewpoints on a prototype holographic display. To this end, we encode the measured light intensity from photos into a neural network representation of underlying wavefields. Our method implicitly learns the amplitude and phase surrogates of the underlying incoherent light waves under coherent light display conditions. During playback, the learned model predicts the underlying continuous complex wavefront propagating to arbitrary views to generate holograms."

- id: isca25
  title: "Process Only Where You Look: Hardware and Algorithm Co-optimization for Efficient Gaze-Tracked Image Rendering in Virtual Reality"
  venue: ISCA
  venue_long: 
  year: 2025
  description: 
  project_page: "projects/isca25/index.html"
  github: 
  arxiv: 
  pdf: 
  teaser_vid: 
  image: isca25.png
  bgcolor: 
  tag: "#F6BEFF"
  teaser_image: isca25teaser.png
  yt_emb: 
  video: 
  teaser_text:
  authors:
    - hwang
    - wliu
    - kchen
    - qsun
    - sqzhang
  affiliation_images:
    - nyu.png
  bibtex:
  abstract: "Virtual reality (VR) plays a crucial role in advancing immersive, interactive experiences that transform learning, work, and entertainment by enhancing user engagement and expanding possibilities across various fields. Image rendering is one of the most crucial application in VR, as it produces high-quality, realistic visuals that are vital for maintaining immersive user experiences and preventing visual discomfort or motion sickness. However, the cost of image rendering in VR environment is considerable, primarily due to the demands of high-quality visual experiences from users. This challenge is even greater in real-time applications, where maintaining low latency further increases the complexity of the rendering process. On the other hand, VR devices, such as head-mounted displays (HMDs), are intrinsically linked to human behavior, using insights from perception and cognition to enhance user experience. In this work, we aim to reduce the high computational costs of the rendering process in VR by leveraging natural human eye dynamics and focusing on processing only where you look (POLO). This involves co-optimizing AI algorithms with underlying hardware for greater efficiency. We introduce POLONet, an efficient multitask deep learning framework designed to track human eye movements with minimal latency. 
  Integrated with the POLO accelerator as a plug-in for VR HMD SoCs, this approach significantly lowers image rendering costs, achieving up to a 3.9x reduction in end-to-end latency compared to the latest gaze tracking methods."

- id: vr25
  title: "Perceptually-Guided Acoustic ``Foveation''"
  venue: IEEE VR
  doi: 10.1109/VR59515.2025.00069
  year: 2025
  description: We present a perceptual sound source clustering technique.
  project_page: "projects/vr25/index.html"
  pdf: "assets/papers/audio_foveation-1.pdf"
  image: vr25.png
  tag: "#F6BEFF"
  teaser_image: vr25_teaser-modified.png
  teaser_text: "Figure 1: Azimuth-based audio perceptual acuity guided sound source clustering. (a) visualizes our model-predicted human auditory perception of spatial discrimination threshold along azimuth eccentricity in degrees. (b) illustrates the model-derived audio source clustering method. Based on listeners’ heading direction (assuming forward here), we cluster audio sources that are spatially indistinguishable. Clusters were highlighted by colors corresponding to the human’s minimum audible angle. The number of sound sources for each cluster is marked in the figure. Our measurement shows a 53% computational saving at the presented scene."
  authors:
    - xpeng
    - kchen
    - iroman
    - jpbello
    - qsun
    - pchakravarthula
  equal_adv:
    - pchakravarthula
    - qsun
  affiliation_images:
    - nyu.png
    - marl.png
    - UNC-Logo.png
  bibtex: "
    @INPROCEEDINGS{<br>
      <tab>peng25acousticfoveation,<br>
      <tab>author={Peng, Xi and Chen, Kenneth and Roman, Iran and Bello, Juan Pablo and Sun, Qi and Chakravarthula, Praneeth},<br>
      <tab>booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},<br> 
      <tab>title={Perceptually-Guided Acoustic \"Foveation\"},<br> 
      <tab>year={2025},<br>
      <tab>pages={450-460},<br>
      <tab>keywords={Solid modeling;Visualization;Sensitivity;Computational modeling;Virtual environments;Rendering (computer graphics);Acoustics;Real-time systems;Psychophysics;Spatial resolution;Perception;Virtual reality;Mixed/Augmented reality},<br>
      <tab>doi={10.1109/VR59515.2025.00069},<br>
    }
  "
  abstract: "Realistic spatial audio rendering improves immersion in virtual environments. However, the computational complexity of acoustic propagation increases linearly with the number of sources. Consequently, real-time accurate acoustic rendering becomes challenging in highly dynamic scenarios such as virtual and augmented reality (VR/AR). Exploiting the fact that human spatial sensitivity of acoustic sources is not equal at azimuth eccentricities in the horizontal plane, we introduce a perceptually-aware acoustic “foveation” guidance model to the audio rendering pipeline, which can integrate audio sources that are not spatially resolvable by human listeners. To this end, we first conduct a series of psychophysical studies to measure the minimum resolvable audible angular distance under various spatial and background conditions. We leverage this data to derive an azimuth-characterized real-time acoustic foveation algorithm. Numerical analysis and subjective user studies in VR environments demonstrate our method’s effectiveness in significantly reducing acoustic rendering workload, without compromising users’ spatial perception of audio sources. We believe that the presented research will motivate future investigation into the new frontier of modeling and leveraging human multimodal per- ceptual limitations — beyond the extensively studied visual acuity — for designing efficient VR/AR systems."

- id: i3d25
  title: "BlendFusion: Procedural 3D Texturing Assistant with View-Consistent Generative Models"
  venue: I3D
  year: 2025
  poster: assets/poster_i3d.png
  description: "Diffusion models can be used for texturing 3D models."
  project_page: "projects/i3d25/index.html"
  pdf: assets/papers/i3d25.pdf
  image: i3d.png
  tag: "#F6BEFF"
  teaser_image: i3d-teaser.png
  doi: 10.1145/3722564.3728376
  award: Best Poster Award
  hide: true
  teaser_text: "Our BlendFusion generative 3D mesh modeling
framework. (a) shows the overall and zoomed-in 3D scene for
re-texturing, (b) shows the modeled texture after 20 minutes
by a professional designer, and (c) shows the results using our
framework with an inference time of 2 minutes with prompt
`a child drawing style of a cabinet in a cartoony kitchen'."
  authors:
    - qli
    - ftorrens
    - kchen
    - qsun
  affiliation_images:
    - nyu.png
  bibtex: "
  @inproceedings{<br>
    <tab>li2025blendfusion,<br>
    <tab>author = {Li, Qinchan and Torrens, Finley and Chen, Kenneth and Sun, Qi},<br> 
    <tab>title = {BlendFusion: Procedural 3D Texturing Assistant with View-Consistent Generative Models},<br> 
    <tab>year = {2025},<br> 
    <tab>isbn = {9798400718335},<br> 
    <tab>publisher = {Association for Computing Machinery},<br> 
    <tab>address = {New York, NY, USA},<br> 
    <tab>url = {https://doi.org/10.1145/3722564.3728376},<br> 
    <tab>doi = {10.1145/3722564.3728376},<br> 
    <tab>booktitle = {Companion Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games},<br> 
    <tab>articleno = {3},<br> 
    <tab>numpages = {3},<br> 
    <tab>series = {I3D Companion '25}<br> 
  }
  "
  abstract: "Modeling 3D assets is a universal in various applications, including animation and game development. However, a key challenge
lies in the labor-intensive task of 3D texturing, where creators
must repeatedly update textures to align with modified geometric shapes on the fly. This iterative workflow makes 3D texturing
significantly more cumbersome and less efficient than 2D image
painting. To address this, we introduce BlendFusion, an interactive
framework that leverages generative diffusion models to streamline
3D texturing. Unlike existing systems that generate textures from
scratch, BlendFusion integrates the procedural nature of texturing
by incorporating multi-view projection to guide the generation
process, enhancing stylistic alignment with the creator's intent.
Experimental results demonstrate the robustness and consistency
of BlendFusion across both objective and subjective evaluations."

- id: sig24
  title: "<b style='color:#4CBB17;'>PEA-PODs</b>: <b style='color:#4CBB17;'>P</b>erceptual <b style='color:#4CBB17;'>E</b>valuation of <b style='color:#4CBB17;'>A</b>lgorithms for <b style='color:#4CBB17;'>P</b>ower <b style='color:#4CBB17;'>O</b>ptimization in XR <b style='color:#4CBB17;'>D</b>isplay<b style='color:#4CBB17;'>s</b>"
  venue: SIGGRAPH
  venue_long:
  doi: 10.1145/3658126
  year: 2024
  showyear: 2024
  highlightvideo: peapods.mp4
  description: "We conduct user studies to evaluate display power optimization algorithms."
  project_page: "projects/sig24/index.html"
  github: NYU-ICL/pea-pods.git
  poster: assets/PEAPODs-Poster.pdf
  pdf: assets/papers/peapods_manuscript_Part1.pdf
  supp: assets/papers/peapods_manuscript_Part2.pdf
  award: Best Paper Award (Honorable Mention)
  award_link: https://blog.siggraph.org/2024/06/siggraph-2024-technical-papers-awards-best-papers-honorable-mentions-and-test-of-time.html/
  image: pea-pods-teaser2.png
  bgcolor: True
  tag: "#F6BEFF"
  teaser_image: peapods_teaser.png
  yt_emb: https://www.youtube.com/embed/CcITr7fy8Go
  video: https://youtu.be/CcITr7fy8Go
  teaser_text: We studied six power-saving display mapping algorithms. These techniques are used in traditional display power reduction as well as recently-proposed methods for VR displays. Just objectionable difference (JOD) scores provide a unified measure of the magnitude of perceptual impact, and percentage values represent relative display power savings (OLED display shown here). Optimal techniques have low perceptual impact (perceived close to reference in JODs), but provide big power savings.
  authors:
    - kchen
    - twan
    - nmatsuda
    - aninan
    - achapiro
    - qsun
  equal_adv:
    - achapiro
    - qsun
  affiliation_images:
    - nyu.png
    - meta.png
  bibtex: "
  @article{ <br>
    <tab>chen2024peapods,<br>
    <tab>author = {Chen, Kenneth and Wan, Thomas and Matsuda, Nathan and Ninan, Ajit and Chapiro, Alexandre and Sun, Qi},<br>
    <tab>title = {PEA-PODs: Perceptual Evaluation of Algorithms for Power Optimization in XR Displays},<br>
    <tab>year = {2024},<br>
    <tab>issue_date = {July 2024},<br>
    <tab>publisher = {Association for Computing Machinery},<br>
    <tab>address = {New York, NY, USA},<br>
    <tab>volume = {43},<br>
    <tab>number = {4},<br>
    <tab>issn = {0730-0301},<br>
    <tab>url = {https://doi.org/10.1145/3658126},<br>
    <tab>doi = {10.1145/3658126},<br>
    <tab>journal = {ACM Trans. Graph.},<br>
    <tab>month = {jul},<br>
    <tab>articleno = {67},<br>
    <tab>numpages = {17},<br>
  }
  "
  abstract: "Display power consumption is an emerging concern for untethered devices. This goes double for augmented and virtual extended reality (XR) displays, which target high refresh rates and high resolutions while conforming to an ergonomically light form factor.
A number of image mapping techniques have been proposed to extend battery usage. However, there is currently no comprehensive quantitative understanding of how the power savings provided by these methods compare to their impact on visual quality. We set out to answer this question.
<br><br>
To this end, we present a <b>p</b>erceptual <b>e</b>valuation of <b>a</b>lgorithms (PEA) for <b>p</b>ower <b>o</b>ptimization in XR <b>d</b>isplay<b>s</b> (PODs). Consolidating a portfolio of six power-saving display mapping approaches, we begin by performing a large-scale perceptual study to understand the impact of each method on perceived quality in the wild. This results in a unified quality score for each technique, scaled in just-objectionable-difference (JOD) units.
In parallel, each technique is analyzed using hardware-accurate power models.
<br><br>
The resulting JOD-to-Milliwatt transfer function provides a first-of-its-kind look into tradeoffs offered by display mapping techniques, and can be directly employed to make architectural decisions for power budgets on XR displays. 
Finally, we leverage our study data and power models to address important display power applications like the choice of display primary, power implications of eye tracking, and more."

- id: asplos24
  title: "Exploiting Human Color Discrimination for Memory- and Energy-Efficient Image Encoding in Virtual Reality"
  venue: ASPLOS
  # venue_long: ACM International Conference on Architectural Support for Programming Languages and Operating Systems
  doi: 10.1145/3617232.3624860
  year: 2024
  description: "A DRAM traffic compression scheme using peripheral color modulation."
  project_page: "projects/asplos24/index.html"
  github: horizon-research/hvs_vr_encoding.git
  papersite: https://horizon-lab.org/pubs/asplos24-vr.pdf
  image: asplos24-2.png
  tag: "#F6BEFF"
  teaser_image: asplos24teaser.png
  yt_emb: https://www.youtube.com/embed/7TLZIIFo7TI
  video: https://www.youtube.com/7TLZIIFo7TI
  teaser_text: Our algorithm takes a tile of pixels and their corresponding discrimination ellipsoid parameters, and generate an adjusted pixel tile, which then goes through existing BD encoding.
  authors:
    - nujjainkar
    - eshahan
    - kchen
    - bduinkharjav
    - qsun
    - yzhu
  affiliation_images:
    - uofr.png
    - nyulogo.png
  bibtex: "@inproceedings{<br>
    <tab>ujjainkar2024exploiting, <br>
    <tab>author = {Ujjainkar, Nisarg and Shahan, Ethan and Chen, Kenneth and Duinkharjav, Budmonde and Sun, Qi and Zhu, Yuhao}, <br>
    <tab>title = {Exploiting Human Color Discrimination for Memory- and Energy-Efficient Image Encoding in Virtual Reality}, <br>
    <tab>year = {2024}, <br>
    <tab>url = {https://doi.org/10.1145/3617232.3624860}, <br>
    <tab>booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1}, <br>
    <tab>pages = {166–180}, <br>
    <tab>numpages = {15}, <br>
    <tab>series = {ASPLOS '24}<br>
    }"
  abstract: "Virtual Reality (VR) has the potential of becoming the next ubiquitous computing platform. Continued progress in the burgeoning field of VR depends critically on an efficient computing substrate. In particular, DRAM access energy is known to contribute to a significant portion of system energy. Today’s framebuffer compression system alleviates the DRAM traffic by using a numerically lossless compression algorithm. We observe that being numerically lossless is unnecessary to preserve perceptual quality for users. This paper proposes a perceptually lossless, but numerically lossy, system to compress DRAM traffics. Our idea builds on top of long-established psychophysical studies that humans cannot discriminate colors that are close to each other. The discrimination ability becomes even weaker (i.e., more colors are perceptually undistinguishable) in our peripheral vision. Leveraging the color discrimination (in)ability, we propose an algorithm that adjusts pixel colors to minimize the bit encoding cost without introducing visible artifacts. The algorithm is coupled with lightweight architectural support that, in real-time, reduces the DRAM traffic by 66.9% and outperforms existing framebuffer compression mechanisms by up to 20.4%. Psychophysical studies on human participants show that our system introduce little to no perceptual fidelity degradation."

- id: sca23
  title: "Towards Learning and Generating Audience Motion from Video"
  venue: SCA
  doi: 10.1145/3606037.3606839
  venue_long: Poster
  year: 2023
  showyear: 2023
  description: "A dataset of audience videos."
  project_page: "projects/sca23/index.html"
  pdf: assets/papers/sca23-4.pdf
  image: audiences-2.png
  tag: "#F6BEFF"
  teaser_image: datagenpipeline-1.png
  video: images/Audiences_SCA23.mp4
  hide: true
  authors:
    - kchen
    - nbadler
  affiliation_images:
    - Cesium_dark_color.png
  bibtex: "@inproceedings{<br>
  <tab>chen2023towards,<br>
  <tab>author = {Chen, Kenneth and Badler, Norman},<br>
  <tab>title = {Towards Learning and Generating Audience Motion from Video},<br>
  <tab>year = {2023},<br>
  <tab>url = {https://doi.org/10.1145/3606037.3606839},<br>
  <tab>booktitle = {Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation},<br>
  <tab>articleno = {4},<br>
  <tab>numpages = {2},<br>
  <tab>series = {SCA '23}<br>
}"
  abstract: "There has recently been an explosion of interest in creating large-scale shared virtual spaces for multiplayer content. However, rendering player-controllable avatars in real-time creates latency issues when scaling to thousands of players. We introduce a human audience video dataset to support applications in deep learning-based 2D video audience simulation, bypassing the need for background 3D virtual humans. This dataset consists of YouTube videos that depict audiences with diverse lighting conditions, color, dress, and movement patterns. We describe the dataset statistics, our implicit data collection strategy, and audience video extraction pipeline. We apply deep learning tasks on this data based on video prediction techniques, and propose a novel method for 2D audience simulations."


- id: vr-energy-etech
  title: "Imperceptible Color Modulation for Power Saving in VR/AR"
  venue: SIGGRAPH E-Tech
  venue_long: Emerging Technologies Demo
  doi: 10.1145/3588037.3595388
  hide: true
  year: 2023
  description: "A demo of the display setup and model from our SIGGRAPH Asia '22 paper."
  project_page: "projects/vr-energy-etech/index.html"
  papersite: https://www.immersivecomputinglab.org/wp-content/uploads/2023/05/VR_Energy_E_Tech.pdf
  image: etech23-3.jpg
  tag: "#F6BEFF"
  paper_image: vr-energy-etech.png
  teaser_image: etech23teaser.png
  teaser_text: a) We develop a hardware prototype OLED VR display to capture power measurements when displaying different images. b) Participants view a panoramic scene in VR with our filter turned on, and are encouraged to rotate freely to view the scene in a natural manner. c) Filter intensity is increased over 10 seconds. Image credits to VR Gorilla.
  authors:
    - kchen
    - bduinkharjav
    - nujjainkar
    - eshahan
    - atyagi
    - jhe
    - yzhu
    - qsun
  affiliation_images:
    - nyulogo.png
    - uofr.png
  bibtex: "@inproceedings{  <br>
    <tab>chen2023imperceptible,<br>
    <tab>author = {Chen, Kenneth and Duinkharjav, Budmonde and Ujjainkar, Nisarg and Shahan, Ethan and Tyagi, Abhishek and He, Jiayi and Zhu, Yuhao and Sun, Qi},<br>
    <tab>title = {Imperceptible Color Modulation for Power Saving in VR/AR},<br>
    <tab>year = {2023},<br>
    <tab>url = {https://doi.org/10.1145/3588037.3595388},<br>
    <tab>booktitle = {ACM SIGGRAPH 2023 Emerging Technologies},<br>
    <tab>articleno = {8},<br>
    <tab>numpages = {2},<br>
    <tab>series = {SIGGRAPH '23}<br>
  }"
  abstract: "Untethered VR/AR HMDs can only last 2-3 hours on a single charge. Toward resolving this issue, we develop a real-time gaze-contingent power saving filter which modulates peripheral pixel color while preserving visual fidelity. At SIGGRAPH 2023, participants will be able to view a short panoramic video within a VR HMD with our perceptually-aware power saving filter turned on. Participants will also have the opportunity to view the power output of scenes through our power measurement setup."


- id: emg-energy
  title: "Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck Muscle Contraction"
  venue: SIGGRAPH
  doi: 10.1145/3588432.3591495
  year: 2023
  description: "Leveraging EMG devices, we developed a model to predict VR neck muscular workload."
  project_page: "projects/emg-energy/index.html"
  github: NYU-ICL/xr-ergonomics-neck-comfort
  papersite: https://www.immersivecomputinglab.org/wp-content/uploads/2023/05/xr-ergonomics-neck-comfort.pdf
  image: emg-2.png
  tag: "#F6BEFF"
  paper_image: mcl-paper.png
  teaser_image: teasersig23.png
  teaser_text: (a) A VR user chooses between two candidate head motion trajectories of seemingly similar muscular workload for a visual task. (b) Our computational model predicts the user’s potential neck muscle contraction level and thus perceived neck muscle discomfort before the movements happen. 3D asset credits to Mixall, Bizulka, RootMotion at Unity, and shockwavegamez01, joseVG at Sketchfab.
  yt_emb: https://www.youtube.com/embed/XO8VR1tJoaI
  video: https://www.youtube.com/watch?v=XO8VR1tJoaI
  authors:
    - yzhang
    - kchen
    - qsun
  affiliation_images:
    - nyulogo.png
  bibtex: "@inproceedings{<br>
    <tab>zhang2023toward,<br>
    <tab>title={Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck Muscle Contraction},<br>
    <tab>author={Zhang, Yunxiang and Chen, Kenneth and Sun, Qi},<br>
    <tab>booktitle={ACM SIGGRAPH 2023 Conference Proceedings},<br>
    <tab>pages={1–12},<br>
    <tab>year={2023}<br>
}"
  abstract: "Ergonomic-friendly usage is essential to mass and prolonged adoption of virtual/augmented reality (VR/AR) head-mounted displays (HMDs). Unlike conventional displays, VR/AR HMDs unlock users' wide-range, frequent, and natural head movements for viewing. Although neck comfort is inevitably compromised due to HMDs' hardware weight, we still have little quantitative knowledge of the resulting additional muscular workload.
<br><br>
Leveraging electromyography devices, we measure, model, and predict users' neck muscle contraction level while they rotate their heads to interact with surrounding objects in VR. Specifically, learning from the data obtained in our physiological pilot study, we establish a bio-physically inspired model for both stationary and dynamic head status. It 1) models quantified muscle contraction level given a complete head motion trajectory, and 2) predicts potential discomfort before a head movement occurs. We validate our model with a series of objective evaluation and user study. The results demonstrate its prediction accuracy for unseen movements, and capability in reducing muscular efforts and thus discomfort by altering the layout of virtual targets. We hope this research will motivate new ergonomic-centered designs and metrics for VR/AR and interactive computer graphics applications."

- id: vrenergy
  title: "Color-Perception-Guided Display Power Reduction for Virtual Reality"
  affiliation_images:
    - nyulogo.png
    - uofr.png
  venue: SIGGRAPH Asia
  doi: 10.1145/3550454.3555473
  showyear: 2022
  year: 2022
  description: "We measured human sensitivity to peripheral color stimuli via a psychophysical studied, and applied this data to OLED display power optimization."
  project_page: "projects/vrenergy/index.html"
  github: NYU-ICL/vr-power-saver
  papersite: https://www.immersivecomputinglab.org/wp-content/uploads/2024/11/vr_energy_siga_22.pdf
  image: colorfov-2.png
  bgcolor: True
  tag: "#F6BEFF"
  highlightvideo: sigasia.mp4
  teaser_text: "We present a perceptually-guided, real-time, and closed-form model for minimizing the power consumption of untethered VR displays while preserving visual fidelity. We apply a gaze-contingent shader onto the original frame ((a) left) to produce a more power efficient frame ((a) right) while preserving the luminance level and perceptual fidelity during active viewing. The dashed circles indicate the user's gaze. Our method is jointly motivated by prior literature revealing that: i) the power cost of displaying different colors on LEDs may vary significantly, even if the luminance levels remain unchanged [Dong and Zhong 2011]; ii) human color sensitivity decreases in peripheral [Hansen et al. 2009] and active vision [Cohen et al. 2020]. The color palette of the original frame is modulated using our peripheral filter and is shown in (a) for a visual comparison. While the color palettes appear different when gazed upon, an observer cannot discriminate between them when shown in their periphery. (b) visualizes how our model shifts the image's chromatic histograms to minimize the physically-measured power consumption. The blue LEDs consume more energy than the red/green in our experiment display panel. Image credits to Tim Caynes © 2012."
  paper_image: vrenergy_paper.png
  teaser_image: sigasia22_teaser.png
  yt_emb: https://www.youtube.com/embed/bcbG_80TLa0
  video: https://www.youtube.com/watch?v=bcbG_80TLa0
  authors:
    - bduinkharjav
    - kchen
    - atyagi
    - jhe
    - yzhu 
    - qsun
  equal: 
    - bduinkharjav
    - kchen
  equal_adv:
    - yzhu
    - qsun
  bibtex: "@article{<br>
    <tab>duinkharjav2022color,<br>
    <tab>title = {Color-Perception-Guided Display Power Reduction for Virtual Reality},<br>
    <tab>author = {Duinkharjav, Budmonde and Chen, Kenneth and Tyagi, Abhishek and He, Jiayi and Zhu, Yuhao and Sun, Qi},<br>
    <tab>journal = {ACM Trans. Graph. (Proc. SIGGRAPH Asia)},<br>
    <tab>volume = {41},<br>
    <tab>number = {6},<br>
    <tab>pages = {144:1--144:16},<br>
    <tab>year = {2022}<br>
  }"
  abstract: "Battery life is an increasingly urgent challenge for today's untethered VR and AR devices.
However, the power efficiency of head-mounted displays is naturally at odds with growing computational requirements driven by better resolution, refresh rate, and dynamic ranges, all of which reduce the sustained usage time of untethered AR/VR devices.
For instance, Oculus Quest 2, under a fully-charged battery, can sustain only 2 to 3 hours of operation time.
Prior display power reduction techniques mostly target smartphone displays.
Directly applying smartphone display power reduction techniques, however, degrades the visual perception in AR/VR with noticeable artifacts.
For instance, the ``power-saving mode'' on smartphones <em>uniformly</em> lowers the pixel luminance across the display and, as a result, presents an overall darkened visual perception to users if directly applied to VR content.
<br><br>
Our key insight is that VR display power reduction must be cognizant of the gaze-contingent nature of high field-of-view VR displays. To that end, we present a gaze-contingent system that, without degrading luminance, minimizes the display power consumption while presenting an indiscriminable visual perception.
This is enabled by constructing 1) a gaze-contingent color discrimination model through psychophysical studies, and 2) a display power model (with respect to pixel color) through real-device measurements. Critically, due to the careful design decisions made in constructing the two models, our algorithm is cast as a constrained optimization problem with a <em>closed-form</em> solution, which can be implemented as a real-time, image-space shader.
We evaluate our system using a series of psychophysical studies and large-scale analyses on natural images.
Experiment results show that our system reduces the display power by as much as 24% (14% on average) with little to no perceptual fidelity degradation."
