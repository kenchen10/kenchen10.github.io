- id: mlpea
  title: "ML-PEA: Machine Learning-Based Perceptual Algorithms for Display Power Optimization"
  venue: Eurographics
  year: 2026
  description: "A neural net predicts spatially-varying dimming maps for display power optimization."
  project_page: "projects/mlpea/index.html"
  github: NYU-ICL/ML-PEA
  image: MLPEA.png
  tag: "#F6BEFF"
  pdf: assets/papers/eg26.pdf
  supp: assets/papers/paper1054_2_MM1.pdf
  teaser_image: mlpea_teaser.png
  teaser_text: "Our pipeline generates images which consume less power than the original when shown on a display, while minimizing perceptual impact. 
  Here, we show an example of an image generated with our technique compared to the reference and its uniformly dimmed version.
  The corresponding dimming maps are shown in the insets, with the multiplicative scaling factor  presented in the color bar on the right. 
  Note that both the uniformly dimmed image and the image generated with our technique in this figure consume the same amount of display power: 52.1% of the reference."
  authors:
    - kchen
    - nmatsuda
    - twan
    - aninan
    - achapiro
    - qsun
  affiliation_images:
    - nyu.png
    - meta.png
  abstract: "Image processing techniques can be used to modulate the pixel intensities of an image to reduce the power consumption of the display device. A simple example of this consists of uniformly dimming the entire image.
  Such algorithms should strive to minimize the impact on image quality while maximizing power savings.
  Techniques based on heuristics or human perception have been proposed, both for traditional flat panel displays and modern display modalities such as virtual and augmented reality (VR/AR).
  In this paper, we focus on developing and evaluating display power-saving techniques that use machine learning (ML).
  This pipeline was validated via quantitative analysis using metrics and through a subjective study.
  Our results show that participants prefer our technique over a uniform dimming baseline for high target power saving conditions.
  In the future, this work should serve as a template and baseline for future applications of deep learning for display power optimization."

- id: wihdirectview
  title: "Perceptual Impact of Peak Luminance and Contrast in Direct View HDR Display"
  venue: IS&T HVEI
  year: 2026
  description: "A follow-up to our 'What is HDR?' paper, adapted for direct-view displays."
  project_page: "projects/wihdirectview/index.html"
  teaser_image: wihdirectview-teaser.png
  pdf: assets/papers/hvei26.pdf
  teaser_text: "In this plot, we show that the existing standard for HDR, DisplayHDR, is not spaced in equal perceptual units (JODs) as defined by our model.
    Red X marks are equally spaced as evaluated by this model."
  image: wihdirectview.png
  tag: "#F6BEFF"
  authors:
    - kchen
    - yzhang
    - qsun
    - achapiro
  equal:
    - kchen 
    - yzhang
  affiliation_images:
    - nyu.png
    - meta.png
  abstract: "Characterization of a high dynamic range (HDR) display's performance can be largely defined by its contrast and peak luminance. 
  Prior work has studied this question for virtual reality (VR) using a haploscopic HDR setup, but it is not obvious if those results are transferrable to a more traditional viewing setting, 
  such as direct view. In this work, we conducted a study to measure user preference for different contrast and peak luminance parameters in this scenario, and develop a perceptual 
  just-objectionable-difference (JOD) scale to quantify preference scores. This is accomplished by studying contrast and peak luminance conditions across several orders of magnitude, 
  shown on a professional HDR display with peak luminance of 1,000 nits and 1,000,000:1 contrast.
  The data is used to develop a computational model that can drive display design and future standardization of the definition of HDR, in terms of human preference."
  bibtex: ""

- id: fovgs
  title: "Perceptually Guided 3DGS Streaming and Rendering for Virtual Reality"
  venue: IEEE/CVF WACV
  year: 2026
  description: "3DGS for wide FOV displays using a perception-guided LOD framework."
  project_page: "projects/fovgs/index.html"
  image: fovgs.png
  tag: "#F6BEFF"
  teaser_image: fovgs_teaser.png
  github: NYU-ICL/perceptual-3dgs
  pdf: https://www.immersivecomputinglab.org/wp-content/uploads/2025/12/Perceptual-3DGS.pdf
  teaser_text: "The proposed perceptually guided, adaptive level-of-detail 3DGS streaming and rendering framework for virtual reality."
  authors:
    - yzhang
    - shmupparaju
    - kchen
    - jkang
    - xzhang
    - momori
    - karimatsu
    - qsun
  equal:
    - yzhang 
    - shmupparaju
  affiliation_images:
    - nyu.png
    - sony-interactive-entertainment.png
  abstract: "Recent breakthroughs in radiance fields, particularly 3D Gaussian Splatting (3DGS), have unlocked real-time, high fidelity rendering of complex environments, boosting broad
applications. However, the stringent requirements of virtual reality (VR), including high refresh rates, high-resolution
stereo rendering, and limited computing, remain beyond the reach of current 3DGS methods. Meanwhile, the wide field-of-view design of VR displays, which mimics natural human
vision, offers a unique opportunity to exploit the limitations of the human visual system to reduce computation overhead
without compromising perceived rendering quality. To this end, we propose a perception-guided, continuous
level-of-detail (LOD) framework for 3DGS that maximizes perceived quality under given computational resources. We
distill a visual quality metric, which encodes the spatial, temporal, and peripheral characteristics of human visual
perception, into a lightweight, gaze-contingent model that predicts and adaptively modulates the LOD across the
user's visual field based on each region's contributions to perceptual quality. This resource-optimized modulation,
guided by both scene content and user gaze behavior, enables significant runtime acceleration with minimal loss in
perceived visual quality. To support low-power, untethered VR setups, we introduce an edge-cloud rendering frame
work that partially offloads computation to the cloud. The framework self-adapts to any edge device's network bandwidth and compute capabilities without requiring tedious
retraining. Objective metrics and VR user study evidence that, compared to vanilla and foveated LOD baselines, our
method achieves superior trade-offs between computational efficiency and perceptual quality."

- id: geneva
  title: "GeneVA: A Dataset of Human Annotations for Generative Text to Video Artifacts"
  venue: IEEE/CVF WACV
  # venue_long: Winter Conference on Applications of Computer Vision
  year: 2026
  arxiv: 2509.08818
  description: "A large-scale dataset of bounding boxes, ratings, descriptions of artifacts in AI videos."
  project_page: "projects/geneva/index.html"
  image: geneva.png
  tag: "#F6BEFF"
  dataset_vid: https://ai-generated-videos-icl.s3.us-east-1.amazonaws.com/
  dataset_annotations: https://geneva-annotations.s3.us-east-1.amazonaws.com/AnnotatedVideos.json
  teaser_image: geneva_teaser.png
  teaser_text: "We show example annotated bounding boxes for each model (labeled on the right). The bounding boxes are annotated in red, with
their artifact category and user-annotated description below the frames. Video quality (“Overall”) and video-prompt alignment (“Prompt”)
are shown to the left. Summary statistics are shown in the radar plots. Specifically, we show statistics for each artifact category, grouped by
category count, average video-prompt alignment, and average video quality rating given the user-selected artifact categories. This is done for
the three models in our dataset: Sora, VideoCrafter2, and Pika. See additional examples in the Appendix."
  authors:
    - jkang
    - psangkloy
    - mbsilva
    - kchen
    - nlwilliams
    - qsun
  affiliation_images:
    - nyu.png
  abstract: "Video generated by the current state-of-the-art generative models contain undesirable artifacts. We introduce
GeneVA, the first large-scale dataset of human-annotated artifact bounding boxes in AI-generated videos. The dataset
consists of 16,356 AI-generated videos, each labeled by a human annotator with per-frame artifact bounding boxes,
their labels and descriptions, and video quality ratings. A custom data collection pipeline was developed in Prolific,
and a novel taxonomy for spatio-temporal artifacts present in AI-generated videos was defined. The videos were from
the VidProM [41] dataset, with text prompts from this dataset then used to generate an additional subset of videos using
Sora. We trained an artifact detector and caption generator using a pre-trained image-based model, and a custom
temporal fusion module. The dataset can be found at dummylink.com. We hope that datasets like GeneVA will encourage improvements in artifact detection in AI-generated video
towards applications such as deepfake detection."

- id: sig25
  title: "What is HDR? Perceptual Impact of Luminance and Contrast in Immersive Displays"
  venue: ACM SIGGRAPH
  doi: 10.1145/3721238.3730629
  year: 2025
  showyear: 2025
  description: "A perceptual scale to describe human preference for contrast and luminance when viewing HDR."
  project_page: "projects/sig25/index.html"
  highlightvideo: sig25.mp4
  github: NYU-ICL/what-is-hdr.git
  arxiv: 
  poster: assets/WIH-poster.pdf
  poster_preview: assets/WIH-poster.png
  ff: images/sig25_ff.mp4
  pdf: assets/papers/WIH_Part1.pdf
  supp: assets/papers/WIH_Part2.pdf
  talk: https://www.youtube.com/embed/tbyemMp3V0Y
  papersite: 
  teaser_vid: 
  award: 
  award_link: 
  acks: "We thank Ken Koh for creating HDR productivity content and Maurizio Nitti for rendering and designing HDR teddy bear scenes. Thanks to Dennis Pak for designing/constructing the haploscope and the mirror setup. Calibration of the EIZO display could not have been accomplished without the support of Yuta Asano. Thank you to Ben Mills for building the enclosure of our haploscope, for calibration of displays, as well as binocular calibration and alignment of mirrors. Thanks go to Will McCann and Xin Li, who supported the construction of the hardware and mirror fabrication. This project would not have been successful without the support of Fartun Sheygo and Alex Gherman, who conducted the main study, Nour Shoora who organized it, the user study participants for their time, and John Hill and Romain Bachy for help with logistics. Thanks go to Henry Milani for providing a PR-745 for validation of our displays, and Reza Saeedpour for support with using the device. Thank you to Dounia Hammou for providing pointers to HDR video datasets, Professor Rafał Mantiuk for the many discussions related to tone mapping and more, and Daryn Blanc-Goldhammer for comments on our work. We are grateful to Alexis Terterov for conducting the validation study, and EIZO support team for help debugging HDR displays. Thanks to Doug Lanman for discussions. Finally, thank you to Jenna Kang, Niall Williams, and Colin Groth for help with figure style. This work is partially supported by National Science Foundation grant #2225861, and a DARPA ICS program."
  image: WIH-image.png
  bgcolor: True
  tag: "#F6BEFF"
  teaser_image: WIH-teaser.png
  yt_emb: 
  other: '<h3>
        Supplemental Webpage
        <hr id="myhr">
    </h3>

    <p class="text-justify" style="width: 98%; margin-left: auto; margin-right: auto;">
        Our <a class="links3" href="interactive_webpage/index.html">[supplementary webpage]</a> is meant to accomodate the "What is HDR?" paper by showcasing still frames from HDR video stimuli used in our user study. HDR content was tested in Google Chrome on a Macbook Pro M3, and can only be viewed accurately on an HDR display.
    </p>'
  video: images/wih_supp_video.mp4
  teaser_text: "A model to predict perceptual impact (in Just-Objectionable-Differences, or JODs) is derived from HDR preference data for combinations of display
contrast and peak luminance, with predictions visualized as a heatmap (left). In this plot, the baseline 0 JOD condition is set to values similar to commercially-
available VR displays: 100 nits peak luminance and 64:1 contrast. In addition, we simulate 3 displays with different dynamic ranges. Our model allows us
to examine the perceived improvement from increased peak luminance and contrast. For example, both display 2 and 3 provide a 1 JOD improvement
over display 1. Note that HDR content cannot be displayed in a PDF format, so all images in this manuscript are tone-mapped for presentation. See our
supplementary webpage for representative content."
  authors:
    - kchen
    - nmatsuda
    - jmcelvain
    - yzhao
    - twan
    - qsun
    - achapiro
  affiliation_images:
    - nyu.png
    - meta.png
  equal_adv:
    - achapiro
    - qsun
  bibtex: "
  @inproceedings{<br>
    <tab>chen2025whatishdr,<br>
    <tab>author = {Chen, Kenneth and Matsuda, Nathan and McElvain, Jon and Zhao, Yang and Wan, Thomas and Sun, Qi and Chapiro, Alexandre},<br>
    <tab>title = {What is HDR? Perceptual Impact of Luminance and Contrast in Immersive Displays},<br>
    <tab>year = {2025},<br>
    <tab>isbn = {9798400715402},<br>
    <tab>publisher = {Association for Computing Machinery},<br>
    <tab>address = {New York, NY, USA},<br>
    <tab>url = {https://doi.org/10.1145/3721238.3730629},<br>
    <tab>doi = {10.1145/3721238.3730629},<br>
    <tab>booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers},<br>
    <tab>articleno = {40},<br>
    <tab>numpages = {11},<br>
    <tab>keywords = {High Dynamic Range, Displays, Visual Perception, Virtual Reality},<br>
    <tab>series = {SIGGRAPH Conference Papers '25}<br>
  }
  "
  abstract: "The contrast and luminance capabilities of a display are central to the quality
of the image. High dynamic range (HDR) displays have high luminance and
contrast, but it can be difficult to ascertain whether a given set of characteristics qualifies for this label. This is especially unclear for new display modes,
such as virtual reality (VR). This paper studies the perceptual impact of peak
luminance and contrast of a display, including characteristics and use cases
representative of VR. To achieve this goal, we first developed a haploscope
testbed prototype display capable of achieving 1k nits peak luminance
and 1M:1 contrast with high precision. We then collected a novel HDR
video dataset targetting VR-relevant content types. We also implemented
custom tone mapping operators to map between display parameter sets.
Finally, we collected subjective preference data spanning 3 orders of magnitude in each dimension. Our data was used to fit a model, which was
validated using a subjective study on an HDR VR prototype headmounted
display (HMD). Our model helps provide guidance for future display design,
and helps standardize the understanding of HDR."

- id: imggs
  title: "Image-GS: Content-Adaptive Image Representation via 2D Gaussians"
  venue: ACM SIGGRAPH
  year: 2025
  highlightvideo: imggs.mp4
  video: images/imggs_ff.mp4
  description: "2D gaussians can serve as an effective technique for low bit rate image encoding."
  project_page: "projects/imggs/index.html"
  github: NYU-ICL/image-gs
  arxiv: 2407.01866
  image: imggs.png
  tag: "#F6BEFF"
  media:
    - id: twominpapers
      link: https://www.youtube.com/watch?v=_WjU5d26Cc4
      img: twominpapersbanner.jpg
    - id: learnopencv
      link: https://learnopencv.com/image-gs-image-reconstruction-using-2d-gaussians/
      img: learnOpenCV.png
  doi: 10.1145/3721238.3730596
  teaser_image: imggs_teaser.png
  bibtex: "
  @inproceedings{<br>
    <tab>zhang2025imggs,<br>
    <tab>author = {Zhang, Yunxiang and Li, Bingxuan and Kuznetsov, Alexandr and Jindal, Akshay and Diolatzis, Stavros and Chen, Kenneth and Sochenov, Anton and Kaplanyan, Anton and Sun, Qi},<br>
    <tab>title = {Image-GS: Content-Adaptive Image Representation via 2D Gaussians},<br>
    <tab>year = {2025},<br>
    <tab>isbn = {9798400715402},<br>
    <tab>publisher = {Association for Computing Machinery},<br>
    <tab>address = {New York, NY, USA},<br>
    <tab>url = {https://doi.org/10.1145/3721238.3730596},<br>
    <tab>doi = {10.1145/3721238.3730596},<br>
    <tab>booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers},<br>
    <tab>articleno = {102},<br>
    <tab>numpages = {11},<br>
    <tab>keywords = {Image and texture representation},<br>
    <tab>series = {SIGGRAPH Conference Papers '25}<br>
  }
  "
  teaser_text: "Content-adaptive image representation with Image-GS. Leveraging a tailored differentiable renderer, Image-GS adaptively distributes and progressively
optimizes a set of 2D Gaussians to fit a target image. Image-GS shows high memory & computation e!iciency, supports fast random pixel access, and o!ers a
natural level of detail. (a) shows the learned Gaussian position distribution (green dots); 20% of Gaussians are plotted for better visibility. (b) Compared to
alternative methods, Image-GS's content-adaptive nature enables it to wisely allocate resources based on the local signal complexity and preserve fine image
details with higher fidelity. The insets visualize the corresponding error images, with brighter colors indicating higher errors."
  authors:
    - yzhang
    - bli
    - akuznetsov
    - ajindal
    - sdiolatzis
    - kchen
    - asochenov
    - akaplanyan
    - qsun
  equal: 
    - yzhang
    - bli
  affiliation_images:
    - nyu.png
    - intel.png
    - amd.png
  abstract: "Neural image representations have emerged as a promising approach for storing and rendering visual data. Combined with learning-based work-flows, these novel representations have demonstrated impressive balances
between visual quality and memory footprint. Existing methods along this line, however, often rely on fixed data structures that suboptimally allocate
memory budget or computation-intensive implicit neural models, limiting their adoption in real-time graphics applications.
Inspired by recent advances in radiance field rendering, we introduce Image-GS, an efficient, flexible, and content-adaptive image representation based on anisotropic 2D Gaussians. Image-GS delivers remarkable visual
quality and memory effciency while supporting fast random access and a
natural level-of-detail stack. Leveraging a custom differentiable renderer imlemented via efficient CUDA kernels, Image-GS reconstructs target images by adaptively allocating and progressively optimizing a set of 2D Gaussians.
Our method achieves superior visual fidelity over state-of-the-art neural image representations across diverse images and textures. Notably, Image-GS exhibits linear scaling in memory and computational requirements relative to the number of Gaussians, offering a flexible trade-off between fidelity and
run-time efficiency, which we demonstrate in machine vision and image restoration tasks."

- id: catimage
  title: "Cost-Aware Routing for Efficient Text-To-Image Generation"
  venue: arxiv
  year: 2025
  description: "Routing text prompts to different text-to-image models can save on compute time."
  project_page: "projects/catimage/index.html"
  github: 
  arxiv: 2506.14753
  image: catimg.png
  tag: "#F6BEFF"
  teaser_image: catimg_teaser.png
  bibtex: ""
  teaser_text: "Two input prompts that require different denoising steps to ensure quality. As shown in (c),
prompt (a) only requires a small number of denoising steps to reach a high CLIPScore. By contrast,
the more complex prompt (b) requires over 100 steps to reach a similar quality. Key to our proposed
CATImage is to allocate an appropriate amount of computation for each prompt, so that the
overall computational cost is reduced while the quality remains the same."
  authors:
    - qli
    - kchen
    - csu
    - wjitkrittum
    - qsun
    - psangkloy
  affiliation_images:
    - nyu.png
    - Google_2015_logo.png
  abstract: "Diffusion models are well known for their ability to generate a high-fidelity image
for an input prompt through an iterative denoising process. Unfortunately, the high
fidelity also comes at a high computational cost due the inherently sequential generative process. In this work, we seek to optimally balance quality and computational
cost, and propose a framework to allow the amount of computation to vary for each
prompt, depending on its complexity. Each prompt is automatically routed to the
most appropriate text-to-image generation function, which may correspond to a
distinct number of denoising steps of a diffusion model, or a disparate, independent
text-to-image model. Unlike uniform cost reduction techniques (e.g., distillation,
model quantization), our approach achieves the optimal trade-off by learning to
reserve expensive choices (e.g., 100+ denoising steps) only for a few complex
prompts, and employ more economical choices (e.g., small distilled model) for less
sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB
that by learning to route to nine already-trained text-to-image models, our approach
is able to deliver an average quality that is higher than that achievable by any of
these models alone."

- id: oe25
  title: "View Synthesis for 3D Computer-Generated Holograms Using Deep Neural Fields"
  venue: Optics Express
  venue_long: 
  year: 2025
  showyear:
  description: "Neural networks are trained to predict 3D holograms at novel scene views."
  project_page: "projects/oe25/index.html"
  github: 
  doi: 10.1364/OE.559364
  arxiv: 
  poster: 
  pdf: assets/papers/oe-33-9-19399.pdf
  supp: assets/papers/7444817.pdf
  papersite: 
  teaser_vid: 
  award: 
  award_link: 
  image: nhf.png
  tag: "#F6BEFF"
  teaser_image: nhf_teaser.png
  yt_emb: 
  video: images/supp-view-interp_s.mp4
  teaser_text: "(A) Camera captures are used to optimize a scene representation model. 
        (B) Light field elemental views are rendered by iteratively evaluating the radiance field at uniformly spaced camera positions.
        Epipolar slices show that the scene representation model can synthesize new views with high image quality.
        (C) The light field is converted to a complex wavefront by computing the inverse of the short-time Fourier transform (iSTFT).
        (D) The model weights of a CNN are optimized by backpropagating the errors of a focal stack loss.
        The ASM is used to simulate wavefront propagation at different distances from a reference plane.
        (E) Insets of model predictions for four test views are shown, with near and far focus."
  authors:
    - kchen
    - awen
    - yzhang
    - pchakravarthula
    - qsun
  affiliation_images:
    - nyu.png
    - UNC-Logo.png
  bibtex: "
    @article{<br>
      <tab>chen2025neuralholographicfields,<br>
      <tab>author = {Kenneth Chen and Anzhou Wen and Yunxiang Zhang and Praneeth Chakravarthula and Qi Sun},<br>
      <tab>journal = {Opt. Express},<br>
      <tab>keywords = {Holographic displays; Imaging techniques; Light propagation; Neural networks; Spatial light modulators; Wave propagation},<br>
      <tab>number = {9},<br>
      <tab>pages = {19399--19408},<br>
      <tab>publisher = {Optica Publishing Group},<br>
      <tab>title = {View synthesis for 3D computer-generated holograms using deep neural fields},<br>
      <tab>volume = {33},<br>
      <tab>month = {May},<br>
      <tab>year = {2025},<br>
      <tab>url = {https://opg.optica.org/oe/abstract.cfm?URI=oe-33-9-19399},<br>
      <tab>doi = {10.1364/OE.559364}<br>
    }
  "
  abstract: "Computer-generated holography (CGH) simulates the propagation and interference of complex light waves, allowing it to reconstruct realistic images captured from a specific viewpoint by solving the corresponding Maxwell equations.
However, in applications such as virtual and augmented reality, viewers should freely observe holograms from arbitrary viewpoints, much as how we naturally see the physical world.  
In this work, we train a neural network to generate holograms at any view in a scene.
Our result is the Neural Holographic Field: the first artificial-neural-network-based representation for light wave propagation in free space and transform sparse 2D photos into holograms that are not only 3D but also freely viewable from any perspective. We demonstrate by visualizing various smartphone-captured scenes from arbitrary six-degree-of-freedom viewpoints on a prototype holographic display. To this end, we encode the measured light intensity from photos into a neural network representation of underlying wavefields. Our method implicitly learns the amplitude and phase surrogates of the underlying incoherent light waves under coherent light display conditions. During playback, the learned model predicts the underlying continuous complex wavefront propagating to arbitrary views to generate holograms."

- id: isca25
  title: "Process Only Where You Look: Hardware and Algorithm Co-optimization for Efficient Gaze-Tracked Image Rendering in Virtual Reality"
  venue: ACM/IEEE ISCA
  venue_long: 
  year: 2025
  description: Hardware-software co-optimization of VR eye tracking.
  project_page: "projects/isca25/index.html"
  github: 
  doi: 10.1145/3695053.3731110
  pdf: "assets/papers/ISCA_2025_POLO_final.pdf"
  teaser_vid: 
  image: isca25.png
  bgcolor: 
  tag: "#F6BEFF"
  teaser_image: isca25teaser.png
  yt_emb: 
  video: 
  teaser_text:
  bibtex: "
  @inproceedings{<br>
    <tab>wang2025polo,<br>
    <tab>author = {Wang, Haiyu and Liu, Wenxuan and Chen, Kenneth and Sun, Qi and Zhang, Sai Qian},<br>
    <tab>title = {Process Only Where You Look: Hardware and Algorithm Co-optimization for Efficient Gaze-Tracked Foveated Rendering in Virtual Reality},<br>
    <tab>year = {2025},<br>
    <tab>isbn = {9798400712616},<br>
    <tab>publisher = {Association for Computing Machinery},<br>
    <tab>address = {New York, NY, USA},<br>
    <tab>url = {https://doi.org/10.1145/3695053.3731110},<br>
    <tab>doi = {10.1145/3695053.3731110},<br>
    <tab>booktitle = {Proceedings of the 52nd Annual International Symposium on Computer Architecture},<br>
    <tab>pages = {344-358},<br>
    <tab>numpages = {15},<br>
    <tab>keywords = {Foveated Rendering, Gaze Tracking, Saccade Detection, Hardware Accelerator, Virtual Reality},<br>
    <tab>series = {ISCA '25}<br>
  }
  "
  authors:
    - hwang
    - wliu
    - kchen
    - qsun
    - sqzhang
  affiliation_images:
    - nyu.png
  abstract: "Virtual reality (VR) plays a crucial role in advancing immersive, interactive experiences that transform learning, work, and entertainment by enhancing user engagement and expanding possibilities across various fields. Image rendering is one of the most crucial application in VR, as it produces high-quality, realistic visuals that are vital for maintaining immersive user experiences and preventing visual discomfort or motion sickness. However, the cost of image rendering in VR environment is considerable, primarily due to the demands of high-quality visual experiences from users. This challenge is even greater in real-time applications, where maintaining low latency further increases the complexity of the rendering process. On the other hand, VR devices, such as head-mounted displays (HMDs), are intrinsically linked to human behavior, using insights from perception and cognition to enhance user experience. 
  <br><br>
  In this work, we aim to reduce the high computational costs of the rendering process in VR by leveraging natural human eye dynamics and focusing on processing only where you look (POLO). This involves co-optimizing AI algorithms with underlying hardware for greater efficiency. We introduce POLONet, an efficient multitask deep learning framework designed to track human eye movements with minimal latency. 
  Integrated with the POLO accelerator as a plug-in for VR HMD SoCs, this approach significantly lowers image rendering costs, achieving up to a 3.9x reduction in end-to-end latency compared to the latest gaze tracking methods."

- id: vr25
  title: "Perceptually-Guided Acoustic ``Foveation''"
  venue: IEEE VR
  doi: 10.1109/VR59515.2025.00069
  year: 2025
  description: A sound source clustering technique based on a model of minimum audible angle.
  project_page: "projects/vr25/index.html"
  pdf: "assets/papers/audio_foveation-1.pdf"
  image: vr25.png
  tag: "#F6BEFF"
  teaser_image: vr25_teaser-modified.png
  teaser_text: "Figure 1: Azimuth-based audio perceptual acuity guided sound source clustering. (a) visualizes our model-predicted human auditory perception of spatial discrimination threshold along azimuth eccentricity in degrees. (b) illustrates the model-derived audio source clustering method. Based on listeners’ heading direction (assuming forward here), we cluster audio sources that are spatially indistinguishable. Clusters were highlighted by colors corresponding to the human’s minimum audible angle. The number of sound sources for each cluster is marked in the figure. Our measurement shows a 53% computational saving at the presented scene."
  authors:
    - xpeng
    - kchen
    - iroman
    - jpbello
    - qsun
    - pchakravarthula
  equal_adv:
    - pchakravarthula
    - qsun
  affiliation_images:
    - nyu.png
    - marl.png
    - UNC-Logo.png
  bibtex: "
    @INPROCEEDINGS{<br>
      <tab>peng25acousticfoveation,<br>
      <tab>author={Peng, Xi and Chen, Kenneth and Roman, Iran and Bello, Juan Pablo and Sun, Qi and Chakravarthula, Praneeth},<br>
      <tab>booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},<br> 
      <tab>title={Perceptually-Guided Acoustic \"Foveation\"},<br> 
      <tab>year={2025},<br>
      <tab>pages={450-460},<br>
      <tab>keywords={Solid modeling;Visualization;Sensitivity;Computational modeling;Virtual environments;Rendering (computer graphics);Acoustics;Real-time systems;Psychophysics;Spatial resolution;Perception;Virtual reality;Mixed/Augmented reality},<br>
      <tab>doi={10.1109/VR59515.2025.00069},<br>
    }
  "
  abstract: "Realistic spatial audio rendering improves immersion in virtual environments. However, the computational complexity of acoustic propagation increases linearly with the number of sources. Consequently, real-time accurate acoustic rendering becomes challenging in highly dynamic scenarios such as virtual and augmented reality (VR/AR). Exploiting the fact that human spatial sensitivity of acoustic sources is not equal at azimuth eccentricities in the horizontal plane, we introduce a perceptually-aware acoustic “foveation” guidance model to the audio rendering pipeline, which can integrate audio sources that are not spatially resolvable by human listeners. To this end, we first conduct a series of psychophysical studies to measure the minimum resolvable audible angular distance under various spatial and background conditions. We leverage this data to derive an azimuth-characterized real-time acoustic foveation algorithm. Numerical analysis and subjective user studies in VR environments demonstrate our method’s effectiveness in significantly reducing acoustic rendering workload, without compromising users’ spatial perception of audio sources. We believe that the presented research will motivate future investigation into the new frontier of modeling and leveraging human multimodal per- ceptual limitations — beyond the extensively studied visual acuity — for designing efficient VR/AR systems."

- id: i3d25
  title: "BlendFusion: Procedural 3D Texturing Assistant with View-Consistent Generative Models"
  venue: ACM I3D
  year: 2025
  poster: assets/poster_i3d.png
  poster_preview: assets/poster_i3d.png
  description: "Diffusion models can be used for texturing 3D models."
  project_page: "projects/i3d25/index.html"
  pdf: assets/papers/i3d25.pdf
  image: i3d.png
  tag: "#F6BEFF"
  teaser_image: i3d-teaser.png
  doi: 10.1145/3722564.3728376
  award: Best Poster Award
  video: images/i3d_video.mp4
  hide: true
  other: '<h3>
            Award
            <hr id="myhr">
        </h3>

        <p class="text-justify" style="width: 98%; margin-left: auto; margin-right: auto;">
        <a href="../../images/i3daward.png">
            <img style="border: 1px solid black;" width="100%" src="../../images/i3daward.png">
        </a>
        </p>'
  teaser_text: "Our BlendFusion generative 3D mesh modeling
framework. (a) shows the overall and zoomed-in 3D scene for
re-texturing, (b) shows the modeled texture after 20 minutes
by a professional designer, and (c) shows the results using our
framework with an inference time of 2 minutes with prompt
`a child drawing style of a cabinet in a cartoony kitchen'."
  authors:
    - qli
    - ftorrens
    - kchen
    - qsun
  affiliation_images:
    - nyu.png
  bibtex: "
  @inproceedings{<br>
    <tab>li2025blendfusion,<br>
    <tab>author = {Li, Qinchan and Torrens, Finley and Chen, Kenneth and Sun, Qi},<br> 
    <tab>title = {BlendFusion: Procedural 3D Texturing Assistant with View-Consistent Generative Models},<br> 
    <tab>year = {2025},<br> 
    <tab>isbn = {9798400718335},<br> 
    <tab>publisher = {Association for Computing Machinery},<br> 
    <tab>address = {New York, NY, USA},<br> 
    <tab>url = {https://doi.org/10.1145/3722564.3728376},<br> 
    <tab>doi = {10.1145/3722564.3728376},<br> 
    <tab>booktitle = {Companion Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games},<br> 
    <tab>articleno = {3},<br> 
    <tab>numpages = {3},<br> 
    <tab>series = {I3D Companion '25}<br> 
  }
  "
  abstract: "Modeling 3D assets is a universal in various applications, including animation and game development. However, a key challenge
lies in the labor-intensive task of 3D texturing, where creators
must repeatedly update textures to align with modified geometric shapes on the fly. This iterative workflow makes 3D texturing
significantly more cumbersome and less efficient than 2D image
painting. To address this, we introduce BlendFusion, an interactive
framework that leverages generative diffusion models to streamline
3D texturing. Unlike existing systems that generate textures from
scratch, BlendFusion integrates the procedural nature of texturing
by incorporating multi-view projection to guide the generation
process, enhancing stylistic alignment with the creator's intent.
Experimental results demonstrate the robustness and consistency
of BlendFusion across both objective and subjective evaluations."

- id: sig24
  title: "<b style='color:#4CBB17;'>PEA-PODs</b>: <b style='color:#4CBB17;'>P</b>erceptual <b style='color:#4CBB17;'>E</b>valuation of <b style='color:#4CBB17;'>A</b>lgorithms for <b style='color:#4CBB17;'>P</b>ower <b style='color:#4CBB17;'>O</b>ptimization in XR <b style='color:#4CBB17;'>D</b>isplay<b style='color:#4CBB17;'>s</b>"
  venue: ACM SIGGRAPH
  venue_long:
  doi: 10.1145/3658126
  year: 2024
  showyear: 2024
  highlightvideo: peapods.mp4
  description: "A subjective evaluation of display power optimization algorithms for wide FOV displays."
  project_page: "projects/sig24/index.html"
  github: NYU-ICL/pea-pods.git
  poster: assets/PEAPODs-Poster.pdf
  poster_preview: assets/PEAPODs-Poster.png
  pdf: assets/papers/peapods_manuscript_Part1.pdf
  supp: assets/papers/peapods_manuscript_Part2.pdf
  award: Best Paper Award (Honorable Mention)
  award_link: https://blog.siggraph.org/2024/06/siggraph-2024-technical-papers-awards-best-papers-honorable-mentions-and-test-of-time.html/
  image: pea-pods-teaser2.png
  bgcolor: True
  tag: "#F6BEFF"
  teaser_image: peapods_teaser.png
  yt_emb: https://www.youtube.com/embed/CcITr7fy8Go
  video: https://youtu.be/CcITr7fy8Go
  other: '<h3>
    Award <a class="links3" href="https://blog.siggraph.org/2024/06/siggraph-2024-technical-papers-awards-best-papers-honorable-mentions-and-test-of-time.html/">[SIGGRAPH Blogpost]</a>
    <hr id="myhr">
</h3>

<p class="text-justify" style="width: 98%; margin-left: auto; margin-right: auto;">
<a href="../../images/award.jpg">
    <embed style="border: 1px solid black;" width="100%" src="../../images/award.jpg">
</a>
</p>
'
  teaser_text: We studied six power-saving display mapping algorithms. These techniques are used in traditional display power reduction as well as recently-proposed methods for VR displays. Just objectionable difference (JOD) scores provide a unified measure of the magnitude of perceptual impact, and percentage values represent relative display power savings (OLED display shown here). Optimal techniques have low perceptual impact (perceived close to reference in JODs), but provide big power savings.
  authors:
    - kchen
    - twan
    - nmatsuda
    - aninan
    - achapiro
    - qsun
  equal_adv:
    - achapiro
    - qsun
  affiliation_images:
    - nyu.png
    - meta.png
  bibtex: "
  @article{ <br>
    <tab>chen2024peapods,<br>
    <tab>author = {Chen, Kenneth and Wan, Thomas and Matsuda, Nathan and Ninan, Ajit and Chapiro, Alexandre and Sun, Qi},<br>
    <tab>title = {PEA-PODs: Perceptual Evaluation of Algorithms for Power Optimization in XR Displays},<br>
    <tab>year = {2024},<br>
    <tab>issue_date = {July 2024},<br>
    <tab>publisher = {Association for Computing Machinery},<br>
    <tab>address = {New York, NY, USA},<br>
    <tab>volume = {43},<br>
    <tab>number = {4},<br>
    <tab>issn = {0730-0301},<br>
    <tab>url = {https://doi.org/10.1145/3658126},<br>
    <tab>doi = {10.1145/3658126},<br>
    <tab>journal = {ACM Trans. Graph.},<br>
    <tab>month = {jul},<br>
    <tab>articleno = {67},<br>
    <tab>numpages = {17},<br>
  }
  "
  abstract: "Display power consumption is an emerging concern for untethered devices. This goes double for augmented and virtual extended reality (XR) displays, which target high refresh rates and high resolutions while conforming to an ergonomically light form factor.
A number of image mapping techniques have been proposed to extend battery usage. However, there is currently no comprehensive quantitative understanding of how the power savings provided by these methods compare to their impact on visual quality. We set out to answer this question.
<br><br>
To this end, we present a <b>p</b>erceptual <b>e</b>valuation of <b>a</b>lgorithms (PEA) for <b>p</b>ower <b>o</b>ptimization in XR <b>d</b>isplay<b>s</b> (PODs). Consolidating a portfolio of six power-saving display mapping approaches, we begin by performing a large-scale perceptual study to understand the impact of each method on perceived quality in the wild. This results in a unified quality score for each technique, scaled in just-objectionable-difference (JOD) units.
In parallel, each technique is analyzed using hardware-accurate power models.
<br><br>
The resulting JOD-to-Milliwatt transfer function provides a first-of-its-kind look into tradeoffs offered by display mapping techniques, and can be directly employed to make architectural decisions for power budgets on XR displays. 
Finally, we leverage our study data and power models to address important display power applications like the choice of display primary, power implications of eye tracking, and more."

- id: asplos24
  title: "Exploiting Human Color Discrimination for Memory- and Energy-Efficient Image Encoding in Virtual Reality"
  venue: ACM ASPLOS
  # venue_long: Architectural Support for Programming Languages and Operating Systems
  doi: 10.1145/3617232.3624860
  year: 2024
  description: "A DRAM traffic compression scheme using peripheral color modulation."
  project_page: "projects/asplos24/index.html"
  github: horizon-research/hvs_vr_encoding.git
  papersite: https://horizon-lab.org/pubs/asplos24-vr.pdf
  image: asplos24-2.png
  tag: "#F6BEFF"
  teaser_image: asplos24teaser.png
  yt_emb: https://www.youtube.com/embed/7TLZIIFo7TI
  video: https://www.youtube.com/7TLZIIFo7TI
  teaser_text: Our algorithm takes a tile of pixels and their corresponding discrimination ellipsoid parameters, and generate an adjusted pixel tile, which then goes through existing BD encoding.
  authors:
    - nujjainkar
    - eshahan
    - kchen
    - bduinkharjav
    - qsun
    - yzhu
  affiliation_images:
    - uofr.png
    - nyulogo.png
  bibtex: "@inproceedings{<br>
    <tab>ujjainkar2024exploiting, <br>
    <tab>author = {Ujjainkar, Nisarg and Shahan, Ethan and Chen, Kenneth and Duinkharjav, Budmonde and Sun, Qi and Zhu, Yuhao}, <br>
    <tab>title = {Exploiting Human Color Discrimination for Memory- and Energy-Efficient Image Encoding in Virtual Reality}, <br>
    <tab>year = {2024}, <br>
    <tab>url = {https://doi.org/10.1145/3617232.3624860}, <br>
    <tab>booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1}, <br>
    <tab>pages = {166–180}, <br>
    <tab>numpages = {15}, <br>
    <tab>series = {ASPLOS '24}<br>
    }"
  abstract: "Virtual Reality (VR) has the potential of becoming the next ubiquitous computing platform. Continued progress in the burgeoning field of VR depends critically on an efficient computing substrate. In particular, DRAM access energy is known to contribute to a significant portion of system energy. Today’s framebuffer compression system alleviates the DRAM traffic by using a numerically lossless compression algorithm. We observe that being numerically lossless is unnecessary to preserve perceptual quality for users. This paper proposes a perceptually lossless, but numerically lossy, system to compress DRAM traffics. Our idea builds on top of long-established psychophysical studies that humans cannot discriminate colors that are close to each other. The discrimination ability becomes even weaker (i.e., more colors are perceptually undistinguishable) in our peripheral vision. Leveraging the color discrimination (in)ability, we propose an algorithm that adjusts pixel colors to minimize the bit encoding cost without introducing visible artifacts. The algorithm is coupled with lightweight architectural support that, in real-time, reduces the DRAM traffic by 66.9% and outperforms existing framebuffer compression mechanisms by up to 20.4%. Psychophysical studies on human participants show that our system introduce little to no perceptual fidelity degradation."

- id: sca23
  title: "Towards Learning and Generating Audience Motion from Video"
  venue: ACM SCA
  doi: 10.1145/3606037.3606839
  venue_long: Poster
  year: 2023
  showyear: 2023
  description: "A dataset of audience videos."
  project_page: "projects/sca23/index.html"
  pdf: assets/papers/sca23-4.pdf
  image: audiences-2.png
  tag: "#F6BEFF"
  teaser_image: datagenpipeline-1.png
  video: images/Audiences_SCA23.mp4
  hide: true
  poster: assets/sca23_poster_tall.pdf
  poster_preview: assets/sca23_poster_tall.png
  authors:
    - kchen
    - nbadler
  affiliation_images:
    - Cesium_dark_color.png
  bibtex: "@inproceedings{<br>
  <tab>chen2023towards,<br>
  <tab>author = {Chen, Kenneth and Badler, Norman},<br>
  <tab>title = {Towards Learning and Generating Audience Motion from Video},<br>
  <tab>year = {2023},<br>
  <tab>url = {https://doi.org/10.1145/3606037.3606839},<br>
  <tab>booktitle = {Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation},<br>
  <tab>articleno = {4},<br>
  <tab>numpages = {2},<br>
  <tab>series = {SCA '23}<br>
}"
  abstract: "There has recently been an explosion of interest in creating large-scale shared virtual spaces for multiplayer content. However, rendering player-controllable avatars in real-time creates latency issues when scaling to thousands of players. We introduce a human audience video dataset to support applications in deep learning-based 2D video audience simulation, bypassing the need for background 3D virtual humans. This dataset consists of YouTube videos that depict audiences with diverse lighting conditions, color, dress, and movement patterns. We describe the dataset statistics, our implicit data collection strategy, and audience video extraction pipeline. We apply deep learning tasks on this data based on video prediction techniques, and propose a novel method for 2D audience simulations."


- id: vr-energy-etech
  title: "Imperceptible Color Modulation for Power Saving in VR/AR"
  venue: ACM SIGGRAPH
  venue_long: Emerging Technologies Demo
  doi: 10.1145/3588037.3595388
  hide: true
  year: 2023
  description: "A demo of the display setup and model from our SIGGRAPH Asia '22 paper."
  project_page: "projects/vr-energy-etech/index.html"
  papersite: https://www.immersivecomputinglab.org/wp-content/uploads/2023/05/VR_Energy_E_Tech.pdf
  image: etech23-3.jpg
  tag: "#F6BEFF"
  paper_image: vr-energy-etech.png
  video: images/etech-video.mp4
  teaser_image: etech23teaser.png
  teaser_text: a) We develop a hardware prototype OLED VR display to capture power measurements when displaying different images. b) Participants view a panoramic scene in VR with our filter turned on, and are encouraged to rotate freely to view the scene in a natural manner. c) Filter intensity is increased over 10 seconds. Image credits to VR Gorilla.
  authors:
    - kchen
    - bduinkharjav
    - nujjainkar
    - eshahan
    - atyagi
    - jhe
    - yzhu
    - qsun
  affiliation_images:
    - nyulogo.png
    - uofr.png
  bibtex: "@inproceedings{  <br>
    <tab>chen2023imperceptible,<br>
    <tab>author = {Chen, Kenneth and Duinkharjav, Budmonde and Ujjainkar, Nisarg and Shahan, Ethan and Tyagi, Abhishek and He, Jiayi and Zhu, Yuhao and Sun, Qi},<br>
    <tab>title = {Imperceptible Color Modulation for Power Saving in VR/AR},<br>
    <tab>year = {2023},<br>
    <tab>url = {https://doi.org/10.1145/3588037.3595388},<br>
    <tab>booktitle = {ACM SIGGRAPH 2023 Emerging Technologies},<br>
    <tab>articleno = {8},<br>
    <tab>numpages = {2},<br>
    <tab>series = {SIGGRAPH '23}<br>
  }"
  abstract: "Untethered VR/AR HMDs can only last 2-3 hours on a single charge. Toward resolving this issue, we develop a real-time gaze-contingent power saving filter which modulates peripheral pixel color while preserving visual fidelity. At SIGGRAPH 2023, participants will be able to view a short panoramic video within a VR HMD with our perceptually-aware power saving filter turned on. Participants will also have the opportunity to view the power output of scenes through our power measurement setup."


- id: emg-energy
  title: "Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck Muscle Contraction"
  venue: ACM SIGGRAPH
  doi: 10.1145/3588432.3591495
  year: 2023
  description: "A model to predict VR neck muscular workload trained on EMG data."
  project_page: "projects/emg-energy/index.html"
  github: NYU-ICL/xr-ergonomics-neck-comfort
  papersite: https://www.immersivecomputinglab.org/wp-content/uploads/2023/05/xr-ergonomics-neck-comfort.pdf
  image: mcl.png
  tag: "#F6BEFF"
  paper_image: mcl-paper.png
  teaser_image: teasersig23.png
  teaser_text: (a) A VR user chooses between two candidate head motion trajectories of seemingly similar muscular workload for a visual task. (b) Our computational model predicts the user’s potential neck muscle contraction level and thus perceived neck muscle discomfort before the movements happen. 3D asset credits to Mixall, Bizulka, RootMotion at Unity, and shockwavegamez01, joseVG at Sketchfab.
  yt_emb: https://www.youtube.com/embed/XO8VR1tJoaI
  video: https://www.youtube.com/watch?v=XO8VR1tJoaI
  authors:
    - yzhang
    - kchen
    - qsun
  affiliation_images:
    - nyulogo.png
  bibtex: "@inproceedings{<br>
    <tab>zhang2023toward,<br>
    <tab>author = {Zhang, Yunxiang and Chen, Kenneth and Sun, Qi},<br>
    <tab>title = {Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck Muscle Contraction},<br>
    <tab>year = {2023},<br>
    <tab>isbn = {9798400701597},<br>
    <tab>publisher = {Association for Computing Machinery},<br>
    <tab>address = {New York, NY, USA},<br>
    <tab>url = {https://doi.org/10.1145/3588432.3591495},<br>
    <tab>doi = {10.1145/3588432.3591495},<br>
    <tab>booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},<br>
    <tab>articleno = {27},<br>
    <tab>numpages = {12},<br>
    <tab>keywords = {Electromyography, Ergonomics, Head-Mounted Display},<br>
    <tab>location = {Los Angeles, CA, USA},<br>
    <tab>series = {SIGGRAPH '23}<br>
    }"
  abstract: "Ergonomic-friendly usage is essential to mass and prolonged adoption of virtual/augmented reality (VR/AR) head-mounted displays (HMDs). Unlike conventional displays, VR/AR HMDs unlock users' wide-range, frequent, and natural head movements for viewing. Although neck comfort is inevitably compromised due to HMDs' hardware weight, we still have little quantitative knowledge of the resulting additional muscular workload.
<br><br>
Leveraging electromyography devices, we measure, model, and predict users' neck muscle contraction level while they rotate their heads to interact with surrounding objects in VR. Specifically, learning from the data obtained in our physiological pilot study, we establish a bio-physically inspired model for both stationary and dynamic head status. It 1) models quantified muscle contraction level given a complete head motion trajectory, and 2) predicts potential discomfort before a head movement occurs. We validate our model with a series of objective evaluation and user study. The results demonstrate its prediction accuracy for unseen movements, and capability in reducing muscular efforts and thus discomfort by altering the layout of virtual targets. We hope this research will motivate new ergonomic-centered designs and metrics for VR/AR and interactive computer graphics applications."

- id: vrenergy
  title: "Color-Perception-Guided Display Power Reduction for Virtual Reality"
  affiliation_images:
    - nyulogo.png
    - uofr.png
  venue: ACM SIGGRAPH Asia
  doi: 10.1145/3550454.3555473
  showyear: 2022
  year: 2022
  description: "VR display power reduction via peripheral color modulation algorithm calibrated on color discrimination data."
  project_page: "projects/vrenergy/index.html"
  github: NYU-ICL/vr-power-saver
  papersite: https://www.immersivecomputinglab.org/wp-content/uploads/2024/11/vr_energy_siga_22.pdf
  image: colorfov-2.png
  bgcolor: True
  tag: "#F6BEFF"
  highlightvideo: sigasia.mp4
  teaser_text: "We present a perceptually-guided, real-time, and closed-form model for minimizing the power consumption of untethered VR displays while preserving visual fidelity. We apply a gaze-contingent shader onto the original frame ((a) left) to produce a more power efficient frame ((a) right) while preserving the luminance level and perceptual fidelity during active viewing. The dashed circles indicate the user's gaze. Our method is jointly motivated by prior literature revealing that: i) the power cost of displaying different colors on LEDs may vary significantly, even if the luminance levels remain unchanged [Dong and Zhong 2011]; ii) human color sensitivity decreases in peripheral [Hansen et al. 2009] and active vision [Cohen et al. 2020]. The color palette of the original frame is modulated using our peripheral filter and is shown in (a) for a visual comparison. While the color palettes appear different when gazed upon, an observer cannot discriminate between them when shown in their periphery. (b) visualizes how our model shifts the image's chromatic histograms to minimize the physically-measured power consumption. The blue LEDs consume more energy than the red/green in our experiment display panel. Image credits to Tim Caynes © 2012."
  paper_image: vrenergy_paper.png
  teaser_image: sigasia22_teaser.png
  yt_emb: https://www.youtube.com/embed/bcbG_80TLa0
  video: https://www.youtube.com/watch?v=bcbG_80TLa0
  authors:
    - bduinkharjav
    - kchen
    - atyagi
    - jhe
    - yzhu 
    - qsun
  equal: 
    - bduinkharjav
    - kchen
  equal_adv:
    - yzhu
    - qsun
  bibtex: "@article{<br>
    <tab>duinkharjav2022color,<br>
    <tab>author = {Duinkharjav, Budmonde and Chen, Kenneth and Tyagi, Abhishek and He, Jiayi and Zhu, Yuhao and Sun, Qi},<br>
    <tab>title = {Color-Perception-Guided Display Power Reduction for Virtual Reality},<br>
    <tab>year = {2022},<br>
    <tab>issue_date = {December 2022},<br>
    <tab>publisher = {Association for Computing Machinery},<br>
    <tab>address = {New York, NY, USA},<br>
    <tab>volume = {41},<br>
    <tab>number = {6},<br>
    <tab>issn = {0730-0301},<br>
    <tab>url = {https://doi.org/10.1145/3550454.3555473},<br>
    <tab>doi = {10.1145/3550454.3555473},<br>
    <tab>journal = {ACM Trans. Graph.},<br>
    <tab>month = nov,<br>
    <tab>articleno = {210},<br>
    <tab>numpages = {16},<br>
    <tab>keywords = {VR/AR, color perception, gaze-contingent rendering, power consumption, visual perception}<br>
    }"
  abstract: "Battery life is an increasingly urgent challenge for today's untethered VR and AR devices.
However, the power efficiency of head-mounted displays is naturally at odds with growing computational requirements driven by better resolution, refresh rate, and dynamic ranges, all of which reduce the sustained usage time of untethered AR/VR devices.
For instance, Oculus Quest 2, under a fully-charged battery, can sustain only 2 to 3 hours of operation time.
Prior display power reduction techniques mostly target smartphone displays.
Directly applying smartphone display power reduction techniques, however, degrades the visual perception in AR/VR with noticeable artifacts.
For instance, the ``power-saving mode'' on smartphones <em>uniformly</em> lowers the pixel luminance across the display and, as a result, presents an overall darkened visual perception to users if directly applied to VR content.
<br><br>
Our key insight is that VR display power reduction must be cognizant of the gaze-contingent nature of high field-of-view VR displays. To that end, we present a gaze-contingent system that, without degrading luminance, minimizes the display power consumption while presenting an indiscriminable visual perception.
This is enabled by constructing 1) a gaze-contingent color discrimination model through psychophysical studies, and 2) a display power model (with respect to pixel color) through real-device measurements. Critically, due to the careful design decisions made in constructing the two models, our algorithm is cast as a constrained optimization problem with a <em>closed-form</em> solution, which can be implemented as a real-time, image-space shader.
We evaluate our system using a series of psychophysical studies and large-scale analyses on natural images.
Experiment results show that our system reduces the display power by as much as 24% (14% on average) with little to no perceptual fidelity degradation."
